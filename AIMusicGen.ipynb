{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import re\n",
    "import json\n",
    "import gc\n",
    "from langdetect import detect\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import Sequence\n",
    "from keras import mixed_precision\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import SparseCategoricalCrossentropy\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Embedding\n",
    "\n",
    "\n",
    "# Enable eager execution in TensorFlow\n",
    "tf.compat.v1.enable_eager_execution()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure TensorFlow GPU settings\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            print(f\"Enabled memory growth for GPU: {gpu}\")\n",
    "    except RuntimeError as e:\n",
    "        print(f\"Failed to set memory growth: {e}\")  # Memory growth must be set before GPUs have been initialized\n",
    "else:\n",
    "    print(\"No GPUs found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Album</th>\n",
       "      <th>Album URL</th>\n",
       "      <th>Artist</th>\n",
       "      <th>Featured Artists</th>\n",
       "      <th>Lyrics</th>\n",
       "      <th>Media</th>\n",
       "      <th>Rank</th>\n",
       "      <th>Release Date</th>\n",
       "      <th>Song Title</th>\n",
       "      <th>Song URL</th>\n",
       "      <th>Writers</th>\n",
       "      <th>Year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Battle of New Orleans</td>\n",
       "      <td>https://genius.com/albums/Johnny-horton/Battle...</td>\n",
       "      <td>Johnny Horton</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Verse 1] In 1814 we took a little trip Along ...</td>\n",
       "      <td>[{'native_uri': 'spotify:track:0dwpdcQkeZqpuoA...</td>\n",
       "      <td>1</td>\n",
       "      <td>1959-04-01</td>\n",
       "      <td>The Battle Of New Orleans</td>\n",
       "      <td>https://genius.com/Johnny-horton-the-battle-of...</td>\n",
       "      <td>[{'api_path': '/artists/561913', 'header_image...</td>\n",
       "      <td>1959.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>That’s All</td>\n",
       "      <td>https://genius.com/albums/Bobby-darin/That-s-all</td>\n",
       "      <td>Bobby Darin</td>\n",
       "      <td>[]</td>\n",
       "      <td>Oh the shark, babe Has such teeth, dear And he...</td>\n",
       "      <td>[{'native_uri': 'spotify:track:3E5ndyOfO6vFDEI...</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mack The Knife</td>\n",
       "      <td>https://genius.com/Bobby-darin-mack-the-knife-...</td>\n",
       "      <td>[{'api_path': '/artists/218851', 'header_image...</td>\n",
       "      <td>1959.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>“Mr Personality’s” 15 Big Hits</td>\n",
       "      <td>https://genius.com/albums/Lloyd-price/Mr-perso...</td>\n",
       "      <td>Lloyd Price</td>\n",
       "      <td>[]</td>\n",
       "      <td>Over and over I tried to prove my love to you ...</td>\n",
       "      <td>[{'provider': 'youtube', 'start': 0, 'type': '...</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Personality</td>\n",
       "      <td>https://genius.com/Lloyd-price-personality-lyrics</td>\n",
       "      <td>[{'api_path': '/artists/355804', 'header_image...</td>\n",
       "      <td>1959.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Greatest Hits Of Frankie Avalon</td>\n",
       "      <td>https://genius.com/albums/Frankie-avalon/The-g...</td>\n",
       "      <td>Frankie Avalon</td>\n",
       "      <td>[]</td>\n",
       "      <td>Hey, Venus! Oh, Venus!  Venus, if you will Ple...</td>\n",
       "      <td>[]</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Venus</td>\n",
       "      <td>https://genius.com/Frankie-avalon-venus-lyrics</td>\n",
       "      <td>[{'api_path': '/artists/1113175', 'header_imag...</td>\n",
       "      <td>1959.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Paul Anka Sings His Big 15</td>\n",
       "      <td>https://genius.com/albums/Paul-anka/Paul-anka-...</td>\n",
       "      <td>Paul Anka</td>\n",
       "      <td>[]</td>\n",
       "      <td>I'm just a lonely boy Lonely and blue I'm all ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Lonely Boy</td>\n",
       "      <td>https://genius.com/Paul-anka-lonely-boy-lyrics</td>\n",
       "      <td>[]</td>\n",
       "      <td>1959.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6495</th>\n",
       "      <td>BZRP Music Sessions</td>\n",
       "      <td>https://genius.com/albums/Bizarrap/Bzrp-music-...</td>\n",
       "      <td>Bizarrap and Shakira</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Intro: Shakira] (Pa' tipos como tú, uh-uh-uh-...</td>\n",
       "      <td>[{'provider': 'youtube', 'start': 0, 'type': '...</td>\n",
       "      <td>96</td>\n",
       "      <td>2023-01-11</td>\n",
       "      <td>Bzrp Music Sessions, Vol. 53</td>\n",
       "      <td>https://genius.com/Bizarrap-and-shakira-shakir...</td>\n",
       "      <td>[{'api_path': '/artists/1405', 'header_image_u...</td>\n",
       "      <td>2023.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6496</th>\n",
       "      <td>Travis Scott - UTOPIA (Русский перевод)</td>\n",
       "      <td>https://genius.com/albums/Genius-russian-trans...</td>\n",
       "      <td>Travis Scott featuring Drake</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Часть I]  [Интро: Drake] Е Напряжение точно р...</td>\n",
       "      <td>[{'provider': 'youtube', 'start': 0, 'type': '...</td>\n",
       "      <td>97</td>\n",
       "      <td>2023-07-28</td>\n",
       "      <td>Meltdown</td>\n",
       "      <td>https://genius.com/Genius-russian-translations...</td>\n",
       "      <td>[]</td>\n",
       "      <td>2023.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6497</th>\n",
       "      <td>L3*</td>\n",
       "      <td>https://genius.com/albums/Latto/L3</td>\n",
       "      <td>Latto featuring Cardi B</td>\n",
       "      <td>[{'api_path': '/artists/621678', 'header_image...</td>\n",
       "      <td>(Go Grizz) Ah (Squat made the beat) What's hap...</td>\n",
       "      <td>[{'provider': 'youtube', 'start': 0, 'type': '...</td>\n",
       "      <td>98</td>\n",
       "      <td>2023-06-02</td>\n",
       "      <td>Put It on da Floor Again</td>\n",
       "      <td>https://genius.com/Latto-put-it-on-da-floor-ag...</td>\n",
       "      <td>[{'api_path': '/artists/163578', 'header_image...</td>\n",
       "      <td>2023.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6498</th>\n",
       "      <td>Born This Way</td>\n",
       "      <td>https://genius.com/albums/Lady-gaga/Born-this-way</td>\n",
       "      <td>Lady Gaga</td>\n",
       "      <td>[]</td>\n",
       "      <td>Money  [Verse 1] Love is just a history that t...</td>\n",
       "      <td>[{'native_uri': 'spotify:track:11BKm0j4eYoCPPp...</td>\n",
       "      <td>99</td>\n",
       "      <td>2011-05-23</td>\n",
       "      <td>Bloody Mary</td>\n",
       "      <td>https://genius.com/Lady-gaga-bloody-mary-lyrics</td>\n",
       "      <td>[{'api_path': '/artists/65581', 'header_image_...</td>\n",
       "      <td>2023.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6499</th>\n",
       "      <td>Bell Bottom Country</td>\n",
       "      <td>https://genius.com/albums/Lainey-wilson/Bell-b...</td>\n",
       "      <td>Lainey Wilson</td>\n",
       "      <td>[]</td>\n",
       "      <td>It was right after senior year Just before the...</td>\n",
       "      <td>[{'provider': 'youtube', 'start': 0, 'type': '...</td>\n",
       "      <td>100</td>\n",
       "      <td>2022-08-12</td>\n",
       "      <td>Watermelon Moonshine</td>\n",
       "      <td>https://genius.com/Lainey-wilson-watermelon-mo...</td>\n",
       "      <td>[{'api_path': '/artists/1149083', 'header_imag...</td>\n",
       "      <td>2023.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6500 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        Album  \\\n",
       "0                       Battle of New Orleans   \n",
       "1                                  That’s All   \n",
       "2              “Mr Personality’s” 15 Big Hits   \n",
       "3         The Greatest Hits Of Frankie Avalon   \n",
       "4                  Paul Anka Sings His Big 15   \n",
       "...                                       ...   \n",
       "6495                      BZRP Music Sessions   \n",
       "6496  Travis Scott - UTOPIA (Русский перевод)   \n",
       "6497                                      L3*   \n",
       "6498                            Born This Way   \n",
       "6499                      Bell Bottom Country   \n",
       "\n",
       "                                              Album URL  \\\n",
       "0     https://genius.com/albums/Johnny-horton/Battle...   \n",
       "1      https://genius.com/albums/Bobby-darin/That-s-all   \n",
       "2     https://genius.com/albums/Lloyd-price/Mr-perso...   \n",
       "3     https://genius.com/albums/Frankie-avalon/The-g...   \n",
       "4     https://genius.com/albums/Paul-anka/Paul-anka-...   \n",
       "...                                                 ...   \n",
       "6495  https://genius.com/albums/Bizarrap/Bzrp-music-...   \n",
       "6496  https://genius.com/albums/Genius-russian-trans...   \n",
       "6497                 https://genius.com/albums/Latto/L3   \n",
       "6498  https://genius.com/albums/Lady-gaga/Born-this-way   \n",
       "6499  https://genius.com/albums/Lainey-wilson/Bell-b...   \n",
       "\n",
       "                            Artist  \\\n",
       "0                    Johnny Horton   \n",
       "1                      Bobby Darin   \n",
       "2                      Lloyd Price   \n",
       "3                   Frankie Avalon   \n",
       "4                        Paul Anka   \n",
       "...                            ...   \n",
       "6495          Bizarrap and Shakira   \n",
       "6496  Travis Scott featuring Drake   \n",
       "6497       Latto featuring Cardi B   \n",
       "6498                     Lady Gaga   \n",
       "6499                 Lainey Wilson   \n",
       "\n",
       "                                       Featured Artists  \\\n",
       "0                                                    []   \n",
       "1                                                    []   \n",
       "2                                                    []   \n",
       "3                                                    []   \n",
       "4                                                    []   \n",
       "...                                                 ...   \n",
       "6495                                                 []   \n",
       "6496                                                 []   \n",
       "6497  [{'api_path': '/artists/621678', 'header_image...   \n",
       "6498                                                 []   \n",
       "6499                                                 []   \n",
       "\n",
       "                                                 Lyrics  \\\n",
       "0     [Verse 1] In 1814 we took a little trip Along ...   \n",
       "1     Oh the shark, babe Has such teeth, dear And he...   \n",
       "2     Over and over I tried to prove my love to you ...   \n",
       "3     Hey, Venus! Oh, Venus!  Venus, if you will Ple...   \n",
       "4     I'm just a lonely boy Lonely and blue I'm all ...   \n",
       "...                                                 ...   \n",
       "6495  [Intro: Shakira] (Pa' tipos como tú, uh-uh-uh-...   \n",
       "6496  [Часть I]  [Интро: Drake] Е Напряжение точно р...   \n",
       "6497  (Go Grizz) Ah (Squat made the beat) What's hap...   \n",
       "6498  Money  [Verse 1] Love is just a history that t...   \n",
       "6499  It was right after senior year Just before the...   \n",
       "\n",
       "                                                  Media  Rank Release Date  \\\n",
       "0     [{'native_uri': 'spotify:track:0dwpdcQkeZqpuoA...     1   1959-04-01   \n",
       "1     [{'native_uri': 'spotify:track:3E5ndyOfO6vFDEI...     2          NaN   \n",
       "2     [{'provider': 'youtube', 'start': 0, 'type': '...     3          NaN   \n",
       "3                                                    []     4          NaN   \n",
       "4                                                    []     5          NaN   \n",
       "...                                                 ...   ...          ...   \n",
       "6495  [{'provider': 'youtube', 'start': 0, 'type': '...    96   2023-01-11   \n",
       "6496  [{'provider': 'youtube', 'start': 0, 'type': '...    97   2023-07-28   \n",
       "6497  [{'provider': 'youtube', 'start': 0, 'type': '...    98   2023-06-02   \n",
       "6498  [{'native_uri': 'spotify:track:11BKm0j4eYoCPPp...    99   2011-05-23   \n",
       "6499  [{'provider': 'youtube', 'start': 0, 'type': '...   100   2022-08-12   \n",
       "\n",
       "                        Song Title  \\\n",
       "0        The Battle Of New Orleans   \n",
       "1                   Mack The Knife   \n",
       "2                      Personality   \n",
       "3                            Venus   \n",
       "4                       Lonely Boy   \n",
       "...                            ...   \n",
       "6495  Bzrp Music Sessions, Vol. 53   \n",
       "6496                      Meltdown   \n",
       "6497      Put It on da Floor Again   \n",
       "6498                   Bloody Mary   \n",
       "6499          Watermelon Moonshine   \n",
       "\n",
       "                                               Song URL  \\\n",
       "0     https://genius.com/Johnny-horton-the-battle-of...   \n",
       "1     https://genius.com/Bobby-darin-mack-the-knife-...   \n",
       "2     https://genius.com/Lloyd-price-personality-lyrics   \n",
       "3        https://genius.com/Frankie-avalon-venus-lyrics   \n",
       "4        https://genius.com/Paul-anka-lonely-boy-lyrics   \n",
       "...                                                 ...   \n",
       "6495  https://genius.com/Bizarrap-and-shakira-shakir...   \n",
       "6496  https://genius.com/Genius-russian-translations...   \n",
       "6497  https://genius.com/Latto-put-it-on-da-floor-ag...   \n",
       "6498    https://genius.com/Lady-gaga-bloody-mary-lyrics   \n",
       "6499  https://genius.com/Lainey-wilson-watermelon-mo...   \n",
       "\n",
       "                                                Writers    Year  \n",
       "0     [{'api_path': '/artists/561913', 'header_image...  1959.0  \n",
       "1     [{'api_path': '/artists/218851', 'header_image...  1959.0  \n",
       "2     [{'api_path': '/artists/355804', 'header_image...  1959.0  \n",
       "3     [{'api_path': '/artists/1113175', 'header_imag...  1959.0  \n",
       "4                                                    []  1959.0  \n",
       "...                                                 ...     ...  \n",
       "6495  [{'api_path': '/artists/1405', 'header_image_u...  2023.0  \n",
       "6496                                                 []  2023.0  \n",
       "6497  [{'api_path': '/artists/163578', 'header_image...  2023.0  \n",
       "6498  [{'api_path': '/artists/65581', 'header_image_...  2023.0  \n",
       "6499  [{'api_path': '/artists/1149083', 'header_imag...  2023.0  \n",
       "\n",
       "[6500 rows x 12 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the dataset\n",
    "data_path = \"E:/archive (9)/all_songs_data.csv\"\n",
    "data = pd.read_csv(data_path, sep=',')\n",
    "\n",
    "# Display the first few rows of the dataframe\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lyrics</th>\n",
       "      <th>Song Title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Verse 1] In 1814 we took a little trip Along ...</td>\n",
       "      <td>The Battle Of New Orleans</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Oh the shark, babe Has such teeth, dear And he...</td>\n",
       "      <td>Mack The Knife</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Over and over I tried to prove my love to you ...</td>\n",
       "      <td>Personality</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hey, Venus! Oh, Venus!  Venus, if you will Ple...</td>\n",
       "      <td>Venus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I'm just a lonely boy Lonely and blue I'm all ...</td>\n",
       "      <td>Lonely Boy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6495</th>\n",
       "      <td>[Intro: Shakira] (Pa' tipos como tú, uh-uh-uh-...</td>\n",
       "      <td>Bzrp Music Sessions, Vol. 53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6496</th>\n",
       "      <td>[Часть I]  [Интро: Drake] Е Напряжение точно р...</td>\n",
       "      <td>Meltdown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6497</th>\n",
       "      <td>(Go Grizz) Ah (Squat made the beat) What's hap...</td>\n",
       "      <td>Put It on da Floor Again</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6498</th>\n",
       "      <td>Money  [Verse 1] Love is just a history that t...</td>\n",
       "      <td>Bloody Mary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6499</th>\n",
       "      <td>It was right after senior year Just before the...</td>\n",
       "      <td>Watermelon Moonshine</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6500 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Lyrics  \\\n",
       "0     [Verse 1] In 1814 we took a little trip Along ...   \n",
       "1     Oh the shark, babe Has such teeth, dear And he...   \n",
       "2     Over and over I tried to prove my love to you ...   \n",
       "3     Hey, Venus! Oh, Venus!  Venus, if you will Ple...   \n",
       "4     I'm just a lonely boy Lonely and blue I'm all ...   \n",
       "...                                                 ...   \n",
       "6495  [Intro: Shakira] (Pa' tipos como tú, uh-uh-uh-...   \n",
       "6496  [Часть I]  [Интро: Drake] Е Напряжение точно р...   \n",
       "6497  (Go Grizz) Ah (Squat made the beat) What's hap...   \n",
       "6498  Money  [Verse 1] Love is just a history that t...   \n",
       "6499  It was right after senior year Just before the...   \n",
       "\n",
       "                        Song Title  \n",
       "0        The Battle Of New Orleans  \n",
       "1                   Mack The Knife  \n",
       "2                      Personality  \n",
       "3                            Venus  \n",
       "4                       Lonely Boy  \n",
       "...                            ...  \n",
       "6495  Bzrp Music Sessions, Vol. 53  \n",
       "6496                      Meltdown  \n",
       "6497      Put It on da Floor Again  \n",
       "6498                   Bloody Mary  \n",
       "6499          Watermelon Moonshine  \n",
       "\n",
       "[6500 rows x 2 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop unnecessary columns from the dataframe\n",
    "columns_to_drop = ['Album', 'Album URL', 'Artist', 'Featured Artists', 'Media', \n",
    "                   'Rank', 'Release Date', 'Song URL', 'Writers', 'Year']\n",
    "df = data.drop(columns=columns_to_drop, axis=1)\n",
    "\n",
    "# Display the first few rows of the modified dataframe\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2410"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Delete the original data dataframe to free up memory\n",
    "del data\n",
    "\n",
    "# Run garbage collection to reclaim memory\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 Lyrics  \\\n",
      "0     [Verse 1] In 1814 we took a little trip Along ...   \n",
      "1     Oh the shark, babe Has such teeth, dear And he...   \n",
      "2     Over and over I tried to prove my love to you ...   \n",
      "3     Hey, Venus! Oh, Venus!  Venus, if you will Ple...   \n",
      "4     I'm just a lonely boy Lonely and blue I'm all ...   \n",
      "...                                                 ...   \n",
      "6493  This how it sound when I hit your ho Be like t...   \n",
      "6494  When it comes to hitchin' the boat up Backin' ...   \n",
      "6497  (Go Grizz) Ah (Squat made the beat) What's hap...   \n",
      "6498  Money  [Verse 1] Love is just a history that t...   \n",
      "6499  It was right after senior year Just before the...   \n",
      "\n",
      "                     Song Title  \n",
      "0     The Battle Of New Orleans  \n",
      "1                Mack The Knife  \n",
      "2                   Personality  \n",
      "3                         Venus  \n",
      "4                    Lonely Boy  \n",
      "...                         ...  \n",
      "6493        Peaches & Eggplants  \n",
      "6494           I Wrote the Book  \n",
      "6497   Put It on da Floor Again  \n",
      "6498                Bloody Mary  \n",
      "6499       Watermelon Moonshine  \n",
      "\n",
      "[6231 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Function to detect if the language of the text is English\n",
    "def detect_language(text):\n",
    "    try:\n",
    "        # Detect the language of the text\n",
    "        lang = detect(text)\n",
    "        # Return True if the detected language is English, False otherwise\n",
    "        return lang == 'en'\n",
    "    except:\n",
    "        # Return False if language detection fails\n",
    "        return False\n",
    "\n",
    "# Apply the function to detect language for each row in the 'Lyrics' column\n",
    "english_lyrics_mask = df['Lyrics'].apply(detect_language)\n",
    "\n",
    "# Filter the dataframe to keep only rows with English lyrics\n",
    "df = df[english_lyrics_mask]\n",
    "\n",
    "# Display the resulting dataframe\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_28264\\606952650.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['Lyrics'] = df['Lyrics'].apply(preprocessing_text)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lyrics</th>\n",
       "      <th>Song Title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>verse 1 in 1814 we took a little trip along wi...</td>\n",
       "      <td>The Battle Of New Orleans</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>oh the shark babe has such teeth dear and he s...</td>\n",
       "      <td>Mack The Knife</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>over and over i tried to prove my love to you ...</td>\n",
       "      <td>Personality</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hey venus oh venus  venus if you will please s...</td>\n",
       "      <td>Venus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>im just a lonely boy lonely and blue im all al...</td>\n",
       "      <td>Lonely Boy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6493</th>\n",
       "      <td>this how it sound when i hit your ho be like t...</td>\n",
       "      <td>Peaches &amp; Eggplants</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6494</th>\n",
       "      <td>when it comes to hitchin the boat up backin do...</td>\n",
       "      <td>I Wrote the Book</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6497</th>\n",
       "      <td>go grizz ah squat made the beat whats happenin...</td>\n",
       "      <td>Put It on da Floor Again</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6498</th>\n",
       "      <td>money  verse 1 love is just a history that the...</td>\n",
       "      <td>Bloody Mary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6499</th>\n",
       "      <td>it was right after senior year just before the...</td>\n",
       "      <td>Watermelon Moonshine</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6231 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Lyrics  \\\n",
       "0     verse 1 in 1814 we took a little trip along wi...   \n",
       "1     oh the shark babe has such teeth dear and he s...   \n",
       "2     over and over i tried to prove my love to you ...   \n",
       "3     hey venus oh venus  venus if you will please s...   \n",
       "4     im just a lonely boy lonely and blue im all al...   \n",
       "...                                                 ...   \n",
       "6493  this how it sound when i hit your ho be like t...   \n",
       "6494  when it comes to hitchin the boat up backin do...   \n",
       "6497  go grizz ah squat made the beat whats happenin...   \n",
       "6498  money  verse 1 love is just a history that the...   \n",
       "6499  it was right after senior year just before the...   \n",
       "\n",
       "                     Song Title  \n",
       "0     The Battle Of New Orleans  \n",
       "1                Mack The Knife  \n",
       "2                   Personality  \n",
       "3                         Venus  \n",
       "4                    Lonely Boy  \n",
       "...                         ...  \n",
       "6493        Peaches & Eggplants  \n",
       "6494           I Wrote the Book  \n",
       "6497   Put It on da Floor Again  \n",
       "6498                Bloody Mary  \n",
       "6499       Watermelon Moonshine  \n",
       "\n",
       "[6231 rows x 2 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Function to preprocess text (in this case, lyrics)\n",
    "def preprocessing_text(text):\n",
    "    text = str(text)  # Convert text to string if not already\n",
    "    text = text.lower()  # Convert text to lowercase\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "    return text\n",
    "\n",
    "# Apply the preprocessing function to the 'Lyrics' column\n",
    "df['Lyrics'] = df['Lyrics'].apply(preprocessing_text)\n",
    "\n",
    "# Display the modified dataframe\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84383 3425 148742 14\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# Preprocess the data\n",
    "lyrics = df['Lyrics'].tolist()\n",
    "titles = df['Song Title'].tolist()\n",
    "\n",
    "# Tokenize the lyrics\n",
    "lyrics_tokenizer = Tokenizer()\n",
    "lyrics_tokenizer.fit_on_texts(lyrics)\n",
    "lyrics_vocab_size = len(lyrics_tokenizer.word_index) + 1  # Vocabulary size for lyrics\n",
    "\n",
    "# Tokenize the titles\n",
    "title_tokenizer = Tokenizer()\n",
    "title_tokenizer.fit_on_texts(titles)\n",
    "title_vocab_size = len(title_tokenizer.word_index) + 1  # Vocabulary size for titles\n",
    "\n",
    "# Pad sequences\n",
    "max_lyrics_len = max([len(l.split()) for l in lyrics])  # Maximum length of lyrics sequence\n",
    "max_title_len = max([len(t.split()) for t in titles])  # Maximum length of titles sequence\n",
    "\n",
    "# Display vocabulary sizes and maximum sequence lengths\n",
    "print(f\"Lyrics Vocabulary Size: {lyrics_vocab_size}\")\n",
    "print(f\"Title Vocabulary Size: {title_vocab_size}\")\n",
    "print(f\"Max Lyrics Length: {max_lyrics_len}\")\n",
    "print(f\"Max Title Length: {max_title_len}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming df is already defined and contains your data\n",
    "\n",
    "# Drop rows with NaN values in the 'Lyrics' column\n",
    "df = df.dropna(subset=['Lyrics'])\n",
    "\n",
    "# Save the cleaned dataframe to a CSV file without index column\n",
    "df.to_csv('lyrics_data.csv', index=False)\n",
    "\n",
    "# Confirmation message\n",
    "print(\"Cleaned data saved to 'lyrics_data.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Convert lyrics to sequences of integers and pad them\n",
    "lyrics_sequences = lyrics_tokenizer.texts_to_sequences(lyrics)\n",
    "lyrics_padded = pad_sequences(lyrics_sequences, maxlen=max_lyrics_len, padding='post')\n",
    "\n",
    "# Convert titles to sequences of integers and pad them\n",
    "title_sequences = title_tokenizer.texts_to_sequences(titles)\n",
    "title_padded = pad_sequences(title_sequences, maxlen=max_title_len, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Delete unnecessary variables to free up memory\n",
    "del title_sequences, lyrics_sequences, df\n",
    "\n",
    "# Run garbage collection to reclaim memory\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save lyrics_tokenizer to a pickle file\n",
    "with open('lyrics_tokenizer.pkl', 'wb') as f:\n",
    "    pickle.dump(lyrics_tokenizer, f)\n",
    "\n",
    "# Save title_tokenizer to a pickle file\n",
    "with open('title_tokenizer.pkl', 'wb') as f:\n",
    "    pickle.dump(title_tokenizer, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model):\n",
    "        super(SelfAttention, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # Initialize weight matrices\n",
    "        self.W_q = tf.keras.layers.Dense(d_model)\n",
    "        self.W_k = tf.keras.layers.Dense(d_model)\n",
    "        self.W_v = tf.keras.layers.Dense(d_model)\n",
    "        \n",
    "    def call(self, q, k, v):\n",
    "        # Compute the self attention scores\n",
    "        scores = tf.matmul(self.W_q(q), tf.transpose(self.W_k(k), perm=[0, 2, 1]))\n",
    "        \n",
    "        # Scale and normalize\n",
    "        scaled_scores = scores / tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        attention_weights = tf.nn.softmax(scaled_scores, axis=-1)\n",
    "        \n",
    "        # Apply attention weights to values\n",
    "        output = tf.matmul(attention_weights, self.W_v(v))\n",
    "        \n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_heads, d_model):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        \n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        self.head_size = d_model // num_heads\n",
    "        \n",
    "        # Create multiple SelfAttention heads\n",
    "        self.attention_heads = [SelfAttention(self.head_size) for _ in range(num_heads)]\n",
    "        \n",
    "        # Final dense layer to combine outputs from all heads\n",
    "        self.dense = tf.keras.layers.Dense(d_model)\n",
    "        \n",
    "    def call(self, q, k, v):\n",
    "        # List comprehension to apply each head to q, k, v\n",
    "        head_outputs = [head(q, k, v)[0] for head in self.attention_heads]\n",
    "        \n",
    "        # Concatenate the outputs of all attention heads\n",
    "        concatenated = tf.concat(head_outputs, axis=-1)\n",
    "        \n",
    "        # Apply the final dense layer\n",
    "        output = self.dense(concatenated)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.dense1 = tf.keras.layers.Dense(d_ff, activation=\"relu\")\n",
    "        self.dense2 = tf.keras.layers.Dense(d_model)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        intermediate_output = self.dense1(inputs)\n",
    "        output = self.dense2(intermediate_output)\n",
    "        return output\n",
    "\n",
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_heads, d_model, d_ff):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.multihead_attention = MultiHeadAttention(num_heads, d_model)\n",
    "        self.feed_forward = FeedForward(d_model, d_ff)\n",
    "        self.layer_norm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layer_norm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        \n",
    "    def call(self, inputs, training=False):\n",
    "        # Self-attention mechanism\n",
    "        attn_output = self.multihead_attention(inputs, inputs, inputs)\n",
    "        \n",
    "        # Residual connection and layer normalization\n",
    "        attn_output = self.layer_norm1(inputs + attn_output)\n",
    "        \n",
    "        # Feed-forward neural network\n",
    "        ff_output = self.feed_forward(attn_output)\n",
    "        \n",
    "        # Residual connection and layer normalization\n",
    "        output = self.layer_norm2(attn_output + ff_output)\n",
    "        \n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_heads, d_model, d_ff):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.multihead_attention = MultiHeadAttention(num_heads, d_model)\n",
    "        self.feed_forward = FeedForward(d_model, d_ff)\n",
    "        self.layer_norm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layer_norm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        \n",
    "    def call(self, inputs, training=False):\n",
    "        # Multi-head self-attention\n",
    "        attn_output = self.multihead_attention(inputs, inputs, inputs)\n",
    "        \n",
    "        # Residual connection and layer normalization\n",
    "        attn_output = self.layer_norm1(inputs + attn_output)\n",
    "        \n",
    "        # Feed-forward neural network\n",
    "        ff_output = self.feed_forward(attn_output)\n",
    "        \n",
    "        # Another residual connection and layer normalization\n",
    "        output = self.layer_norm2(attn_output + ff_output)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_heads, d_model, d_ff):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        \n",
    "        # Masked self-attention layer\n",
    "        self.masked_self_attention = MultiHeadAttention(num_heads, d_model)\n",
    "        \n",
    "        # Cross-attention layer with encoder output\n",
    "        self.cross_attention = MultiHeadAttention(num_heads, d_model)\n",
    "        \n",
    "        # Feed-forward neural network layer\n",
    "        self.feed_forward = FeedForward(d_model, d_ff)\n",
    "        \n",
    "        # Layer normalization layers\n",
    "        self.layer_norm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layer_norm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layer_norm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "            \n",
    "    def call(self, inputs, enc_output, training=False):\n",
    "        # Apply masked self-attention\n",
    "        masked_attn_output = self.masked_self_attention(inputs, inputs, inputs)\n",
    "        \n",
    "        # Residual connection and layer normalization\n",
    "        masked_attn_output = self.layer_norm1(inputs + masked_attn_output)\n",
    "        \n",
    "        # Apply cross-attention with encoder output\n",
    "        cross_attn_output = self.cross_attention(masked_attn_output, enc_output, enc_output)\n",
    "        \n",
    "        # Residual connection and layer normalization\n",
    "        cross_attn_output = self.layer_norm2(masked_attn_output + cross_attn_output)\n",
    "        \n",
    "        # Apply feed-forward neural network\n",
    "        ff_output = self.feed_forward(cross_attn_output)\n",
    "        \n",
    "        # Residual connection and layer normalization\n",
    "        output = self.layer_norm3(cross_attn_output + ff_output)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, num_layers, num_heads, d_model, d_ff, input_vocab_size, target_vocab_size, max_seq_len):\n",
    "        super(Transformer, self).__init__()\n",
    "        \n",
    "        # Initialize encoder layers\n",
    "        self.encoder_layers = [EncoderLayer(num_heads, d_model, d_ff) for _ in range(num_layers)]\n",
    "        \n",
    "        # Initialize decoder layers\n",
    "        self.decoder_layers = [DecoderLayer(num_heads, d_model, d_ff) for _ in range(num_layers)]\n",
    "        \n",
    "        # Embedding layer for input and target sequences\n",
    "        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
    "        \n",
    "        # Final dense layer for prediction\n",
    "        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
    "        \n",
    "        # Softmax layer for output\n",
    "        self.softmax = tf.keras.layers.Softmax(axis=-1)\n",
    "        \n",
    "        # Maximum sequence length\n",
    "        self.max_seq_len = max_seq_len\n",
    "        \n",
    "    def call(self, inputs, training=False):\n",
    "        enc_inputs, dec_inputs = inputs\n",
    "        \n",
    "        # Encode input sequence\n",
    "        enc_output = self.encode(enc_inputs, training=training)\n",
    "        \n",
    "        # Decode output sequence\n",
    "        dec_output = self.decode(dec_inputs, enc_output, training=training)\n",
    "        \n",
    "        return dec_output\n",
    "\n",
    "    def encode(self, inputs, training=False):\n",
    "        # Embedding and positional encoding\n",
    "        x = self.embedding(inputs)\n",
    "        \n",
    "        # Iterate through encoder layers\n",
    "        for encoder_layer in self.encoder_layers:\n",
    "            x = encoder_layer(x, training=training)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def decode(self, targets, enc_output, training=False):\n",
    "        # Embedding and positional encoding\n",
    "        x = self.embedding(targets)\n",
    "        \n",
    "        # Iterate through decoder layers\n",
    "        for decoder_layer in self.decoder_layers:\n",
    "            x = decoder_layer(x, enc_output, training=training)\n",
    "        \n",
    "        # Final dense layer for prediction\n",
    "        dec_output = self.final_layer(x)\n",
    "        \n",
    "        # Apply softmax activation\n",
    "        dec_output = self.softmax(dec_output)\n",
    "        \n",
    "        return dec_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load lyrics_tokenizer\n",
    "with open('lyrics_tokenizer.pkl', 'rb') as f:\n",
    "    lyrics_tokenizer = pickle.load(f)\n",
    "\n",
    "# Load title_tokenizer\n",
    "with open('title_tokenizer.pkl', 'rb') as f:\n",
    "    title_tokenizer = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 9130\n",
      "(6231, 14) (6231, 99) (6231, 99)\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('lyrics_data.csv')\n",
    "\n",
    "# Function to extract first 100 characters from text\n",
    "def first_100_chars(text):\n",
    "    return text[:100]\n",
    "\n",
    "# Apply the function to the 'Lyrics' column\n",
    "df['Lyrics'] = df['Lyrics'].apply(first_100_chars)\n",
    "\n",
    "# Constants\n",
    "max_lyrics_len = 100\n",
    "max_title_len = 14\n",
    "\n",
    "# Tokenize lyrics using pre-trained lyrics_tokenizer\n",
    "lyrics_sequences = lyrics_tokenizer.texts_to_sequences(df['Lyrics'])\n",
    "lyrics_padded = pad_sequences(lyrics_sequences, maxlen=max_lyrics_len, padding='post')\n",
    "\n",
    "# Tokenize titles using pre-trained title_tokenizer\n",
    "title_sequences = title_tokenizer.texts_to_sequences(df['Song Title'])\n",
    "title_padded = pad_sequences(title_sequences, maxlen=max_title_len, padding='post')\n",
    "\n",
    "# Split the data into inputs and outputs\n",
    "title_inputs = title_padded\n",
    "lyrics_inputs = lyrics_padded[:, :-1]  # Use all but the last token as input\n",
    "lyrics_outputs = lyrics_padded[:, 1:]  # Predict the next token in the sequence\n",
    "\n",
    "# Print shapes for verification\n",
    "print(title_inputs.shape, lyrics_inputs.shape, lyrics_outputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_3\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_3\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ encoder_input       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder_input       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">99</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ transformer_1       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">99</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9140</span>)  │  <span style=\"color: #00af00; text-decoration-color: #00af00\">3,273,396</span> │ encoder_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Transformer</span>)       │                   │            │ decoder_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ encoder_input       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder_input       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m99\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ transformer_1       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m99\u001b[0m, \u001b[38;5;34m9140\u001b[0m)  │  \u001b[38;5;34m3,273,396\u001b[0m │ encoder_input[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mTransformer\u001b[0m)       │                   │            │ decoder_input[\u001b[38;5;34m0\u001b[0m]… │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,273,396</span> (12.49 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m3,273,396\u001b[0m (12.49 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,273,396</span> (12.49 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m3,273,396\u001b[0m (12.49 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define parameters\n",
    "num_layers = 2\n",
    "num_heads = 4\n",
    "d_model = 128\n",
    "d_ff = 512\n",
    "input_vocab_size = 9130\n",
    "target_vocab_size = 9140\n",
    "max_lyrics_len = 99\n",
    "max_title_len = 14\n",
    "\n",
    "# Define the input shapes\n",
    "encoder_input = Input(shape=(max_title_len,), dtype=tf.int32, name=\"encoder_input\")\n",
    "decoder_input = Input(shape=(max_lyrics_len,), dtype=tf.int32, name=\"decoder_input\")\n",
    "\n",
    "# Instantiate the Transformer model\n",
    "transformer = Transformer(num_layers, num_heads, d_model, d_ff, input_vocab_size, target_vocab_size, max_lyrics_len)\n",
    "\n",
    "# Get the outputs by passing the inputs through the transformer model\n",
    "outputs = transformer([encoder_input, decoder_input])\n",
    "\n",
    "# Create the Keras model\n",
    "model = tf.keras.Model(inputs=[encoder_input, decoder_input], outputs=outputs)\n",
    "\n",
    "# Print the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 605ms/step - loss: 4.7883 - learning_rate: 0.0010\n",
      "Epoch 2/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 605ms/step - loss: 1.3992 - learning_rate: 0.0010\n",
      "Epoch 3/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 611ms/step - loss: 1.3658 - learning_rate: 0.0010\n",
      "Epoch 4/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 604ms/step - loss: 1.3186 - learning_rate: 0.0010\n",
      "Epoch 5/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 604ms/step - loss: 1.1836 - learning_rate: 0.0010\n",
      "Epoch 6/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 604ms/step - loss: 1.0284 - learning_rate: 0.0010\n",
      "Epoch 7/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 606ms/step - loss: 0.9072 - learning_rate: 0.0010\n",
      "Epoch 8/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 607ms/step - loss: 0.7865 - learning_rate: 0.0010\n",
      "Epoch 9/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 605ms/step - loss: 0.6947 - learning_rate: 0.0010\n",
      "Epoch 10/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 602ms/step - loss: 0.6012 - learning_rate: 0.0010\n",
      "Epoch 11/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 611ms/step - loss: 0.5238 - learning_rate: 0.0010\n",
      "Epoch 12/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 600ms/step - loss: 0.4557 - learning_rate: 0.0010\n",
      "Epoch 13/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 604ms/step - loss: 0.3937 - learning_rate: 0.0010\n",
      "Epoch 14/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 607ms/step - loss: 0.3407 - learning_rate: 0.0010\n",
      "Epoch 15/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 606ms/step - loss: 0.2937 - learning_rate: 0.0010\n",
      "Epoch 16/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 604ms/step - loss: 0.2555 - learning_rate: 0.0010\n",
      "Epoch 17/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 613ms/step - loss: 0.2238 - learning_rate: 0.0010\n",
      "Epoch 18/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 606ms/step - loss: 0.1994 - learning_rate: 0.0010\n",
      "Epoch 19/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 604ms/step - loss: 0.1805 - learning_rate: 0.0010\n",
      "Epoch 20/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 606ms/step - loss: 0.1630 - learning_rate: 0.0010\n",
      "Epoch 21/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 608ms/step - loss: 0.1509 - learning_rate: 0.0010\n",
      "Epoch 22/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 610ms/step - loss: 0.1409 - learning_rate: 0.0010\n",
      "Epoch 23/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 607ms/step - loss: 0.1317 - learning_rate: 0.0010\n",
      "Epoch 24/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 608ms/step - loss: 0.1220 - learning_rate: 0.0010\n",
      "Epoch 25/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 609ms/step - loss: 0.1182 - learning_rate: 0.0010\n",
      "Epoch 26/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 610ms/step - loss: 0.1124 - learning_rate: 0.0010\n",
      "Epoch 27/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 607ms/step - loss: 0.1084 - learning_rate: 0.0010\n",
      "Epoch 28/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 609ms/step - loss: 0.1026 - learning_rate: 0.0010\n",
      "Epoch 29/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 611ms/step - loss: 0.1013 - learning_rate: 0.0010\n",
      "Epoch 30/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 613ms/step - loss: 0.0979 - learning_rate: 0.0010\n",
      "Epoch 31/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 611ms/step - loss: 0.0964 - learning_rate: 0.0010\n",
      "Epoch 32/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 610ms/step - loss: 0.0936 - learning_rate: 0.0010\n",
      "Epoch 33/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 609ms/step - loss: 0.0902 - learning_rate: 0.0010\n",
      "Epoch 34/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 611ms/step - loss: 0.0882 - learning_rate: 0.0010\n",
      "Epoch 35/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 612ms/step - loss: 0.0877 - learning_rate: 0.0010\n",
      "Epoch 36/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 612ms/step - loss: 0.0859 - learning_rate: 0.0010\n",
      "Epoch 37/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 615ms/step - loss: 0.0838 - learning_rate: 0.0010\n",
      "Epoch 38/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 612ms/step - loss: 0.0830 - learning_rate: 0.0010\n",
      "Epoch 39/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 614ms/step - loss: 0.0821 - learning_rate: 0.0010\n",
      "Epoch 40/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 613ms/step - loss: 0.0814 - learning_rate: 0.0010\n",
      "Epoch 41/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 613ms/step - loss: 0.0780 - learning_rate: 0.0010\n",
      "Epoch 42/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 614ms/step - loss: 0.0775 - learning_rate: 0.0010\n",
      "Epoch 43/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 615ms/step - loss: 0.0786 - learning_rate: 0.0010\n",
      "Epoch 44/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 615ms/step - loss: 0.0791 - learning_rate: 0.0010\n",
      "Epoch 45/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 618ms/step - loss: 0.0771 - learning_rate: 0.0010\n",
      "Epoch 46/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 614ms/step - loss: 0.0763 - learning_rate: 0.0010\n",
      "Epoch 47/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 614ms/step - loss: 0.0772 - learning_rate: 0.0010\n",
      "Epoch 48/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 613ms/step - loss: 0.0757 - learning_rate: 0.0010\n",
      "Epoch 49/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 615ms/step - loss: 0.0746 - learning_rate: 0.0010\n",
      "Epoch 50/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 614ms/step - loss: 0.0740 - learning_rate: 0.0010\n",
      "Epoch 51/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 616ms/step - loss: 0.0755 - learning_rate: 0.0010\n",
      "Epoch 52/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 617ms/step - loss: 0.0735 - learning_rate: 0.0010\n",
      "Epoch 53/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 615ms/step - loss: 0.0740 - learning_rate: 0.0010\n",
      "Epoch 54/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 619ms/step - loss: 0.0739 - learning_rate: 0.0010\n",
      "Epoch 55/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 612ms/step - loss: 0.0727 - learning_rate: 0.0010\n",
      "Epoch 56/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 612ms/step - loss: 0.0723 - learning_rate: 0.0010\n",
      "Epoch 57/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 613ms/step - loss: 0.0717 - learning_rate: 0.0010\n",
      "Epoch 58/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 613ms/step - loss: 0.0737 - learning_rate: 0.0010\n",
      "Epoch 59/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 618ms/step - loss: 0.0719 - learning_rate: 0.0010\n",
      "Epoch 60/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 614ms/step - loss: 0.0700 - learning_rate: 0.0010\n",
      "Epoch 61/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 613ms/step - loss: 0.0695 - learning_rate: 0.0010\n",
      "Epoch 62/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 609ms/step - loss: 0.0699 - learning_rate: 0.0010\n",
      "Epoch 63/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 612ms/step - loss: 0.0699 - learning_rate: 0.0010\n",
      "Epoch 64/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 613ms/step - loss: 0.0703 - learning_rate: 0.0010\n",
      "Epoch 65/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 615ms/step - loss: 0.0693 - learning_rate: 0.0010\n",
      "Epoch 66/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 611ms/step - loss: 0.0695 - learning_rate: 0.0010\n",
      "Epoch 67/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 613ms/step - loss: 0.0693 - learning_rate: 0.0010\n",
      "Epoch 68/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 612ms/step - loss: 0.0686 - learning_rate: 0.0010\n",
      "Epoch 69/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 614ms/step - loss: 0.0681 - learning_rate: 0.0010\n",
      "Epoch 70/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 609ms/step - loss: 0.0674 - learning_rate: 0.0010\n",
      "Epoch 71/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 621ms/step - loss: 0.0683 - learning_rate: 0.0010\n",
      "Epoch 72/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 612ms/step - loss: 0.0674 - learning_rate: 0.0010\n",
      "Epoch 73/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 611ms/step - loss: 0.0680 - learning_rate: 0.0010\n",
      "Epoch 74/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 614ms/step - loss: 0.0669 - learning_rate: 0.0010\n",
      "Epoch 75/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 613ms/step - loss: 0.0680 - learning_rate: 0.0010\n",
      "Epoch 76/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 611ms/step - loss: 0.0655 - learning_rate: 0.0010\n",
      "Epoch 77/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 610ms/step - loss: 0.0675 - learning_rate: 0.0010\n",
      "Epoch 78/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 610ms/step - loss: 0.0649 - learning_rate: 0.0010\n",
      "Epoch 79/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 611ms/step - loss: 0.0653 - learning_rate: 0.0010\n",
      "Epoch 80/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 611ms/step - loss: 0.0642 - learning_rate: 0.0010\n",
      "Epoch 81/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 609ms/step - loss: 0.0639 - learning_rate: 0.0010\n",
      "Epoch 82/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 611ms/step - loss: 0.0641 - learning_rate: 0.0010\n",
      "Epoch 83/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 611ms/step - loss: 0.0648 - learning_rate: 0.0010\n",
      "Epoch 84/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 611ms/step - loss: 0.0658 - learning_rate: 0.0010\n",
      "Epoch 85/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 610ms/step - loss: 0.0637 - learning_rate: 0.0010\n",
      "Epoch 86/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 609ms/step - loss: 0.0634 - learning_rate: 0.0010\n",
      "Epoch 87/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 611ms/step - loss: 0.0643 - learning_rate: 0.0010\n",
      "Epoch 88/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 611ms/step - loss: 0.0619 - learning_rate: 0.0010\n",
      "Epoch 89/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 611ms/step - loss: 0.0638 - learning_rate: 0.0010\n",
      "Epoch 90/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 612ms/step - loss: 0.0641 - learning_rate: 0.0010\n",
      "Epoch 91/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 610ms/step - loss: 0.0634 - learning_rate: 0.0010\n",
      "Epoch 92/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 608ms/step - loss: 0.0635 - learning_rate: 0.0010\n",
      "Epoch 93/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 610ms/step - loss: 0.0630 - learning_rate: 0.0010\n",
      "Epoch 94/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 608ms/step - loss: 0.0648 - learning_rate: 0.0010\n",
      "Epoch 95/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 611ms/step - loss: 0.0635 - learning_rate: 0.0010\n",
      "Epoch 96/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 611ms/step - loss: 0.0635 - learning_rate: 0.0010\n",
      "Epoch 97/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 610ms/step - loss: 0.0626 - learning_rate: 0.0010\n",
      "Epoch 98/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 608ms/step - loss: 0.0612 - learning_rate: 0.0010\n",
      "Epoch 99/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 610ms/step - loss: 0.0617 - learning_rate: 0.0010\n",
      "Epoch 100/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 609ms/step - loss: 0.0618 - learning_rate: 0.0010\n",
      "Epoch 101/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 610ms/step - loss: 0.0623 - learning_rate: 0.0010\n",
      "Epoch 102/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 607ms/step - loss: 0.0620 - learning_rate: 0.0010\n",
      "Epoch 103/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 610ms/step - loss: 0.0609 - learning_rate: 0.0010\n",
      "Epoch 104/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 609ms/step - loss: 0.0611 - learning_rate: 0.0010\n",
      "Epoch 105/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 610ms/step - loss: 0.0620 - learning_rate: 0.0010\n",
      "Epoch 106/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 609ms/step - loss: 0.0625 - learning_rate: 0.0010\n",
      "Epoch 107/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 610ms/step - loss: 0.0609 - learning_rate: 0.0010\n",
      "Epoch 108/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 608ms/step - loss: 0.0610 - learning_rate: 0.0010\n",
      "Epoch 109/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 608ms/step - loss: 0.0604 - learning_rate: 0.0010\n",
      "Epoch 110/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 609ms/step - loss: 0.0595 - learning_rate: 0.0010\n",
      "Epoch 111/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 611ms/step - loss: 0.0598 - learning_rate: 0.0010\n",
      "Epoch 112/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 611ms/step - loss: 0.0596 - learning_rate: 0.0010\n",
      "Epoch 113/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 610ms/step - loss: 0.0595 - learning_rate: 0.0010\n",
      "Epoch 114/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 608ms/step - loss: 0.0586 - learning_rate: 0.0010\n",
      "Epoch 115/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 609ms/step - loss: 0.0585 - learning_rate: 0.0010\n",
      "Epoch 116/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 608ms/step - loss: 0.0600 - learning_rate: 0.0010\n",
      "Epoch 117/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 607ms/step - loss: 0.0601 - learning_rate: 0.0010\n",
      "Epoch 118/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 611ms/step - loss: 0.0589 - learning_rate: 0.0010\n",
      "Epoch 119/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 611ms/step - loss: 0.0598 - learning_rate: 0.0010\n",
      "Epoch 120/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 610ms/step - loss: 0.0604 - learning_rate: 0.0010\n",
      "Epoch 121/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 606ms/step - loss: 0.0605 - learning_rate: 0.0010\n",
      "Epoch 122/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 607ms/step - loss: 0.0615 - learning_rate: 0.0010\n",
      "Epoch 123/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 609ms/step - loss: 0.0610 - learning_rate: 0.0010\n",
      "Epoch 124/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 608ms/step - loss: 0.0580 - learning_rate: 1.0000e-04\n",
      "Epoch 125/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 608ms/step - loss: 0.0492 - learning_rate: 1.0000e-04\n",
      "Epoch 126/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 607ms/step - loss: 0.0447 - learning_rate: 1.0000e-04\n",
      "Epoch 127/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 608ms/step - loss: 0.0424 - learning_rate: 1.0000e-04\n",
      "Epoch 128/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 606ms/step - loss: 0.0416 - learning_rate: 1.0000e-04\n",
      "Epoch 129/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 608ms/step - loss: 0.0402 - learning_rate: 1.0000e-04\n",
      "Epoch 130/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 607ms/step - loss: 0.0397 - learning_rate: 1.0000e-04\n",
      "Epoch 131/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 613ms/step - loss: 0.0389 - learning_rate: 1.0000e-04\n",
      "Epoch 132/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 607ms/step - loss: 0.0387 - learning_rate: 1.0000e-04\n",
      "Epoch 133/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 607ms/step - loss: 0.0380 - learning_rate: 1.0000e-04\n",
      "Epoch 134/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 610ms/step - loss: 0.0382 - learning_rate: 1.0000e-04\n",
      "Epoch 135/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 606ms/step - loss: 0.0378 - learning_rate: 1.0000e-04\n",
      "Epoch 136/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 608ms/step - loss: 0.0377 - learning_rate: 1.0000e-04\n",
      "Epoch 137/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 607ms/step - loss: 0.0377 - learning_rate: 1.0000e-04\n",
      "Epoch 138/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 606ms/step - loss: 0.0378 - learning_rate: 1.0000e-04\n",
      "Epoch 139/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 607ms/step - loss: 0.0371 - learning_rate: 1.0000e-04\n",
      "Epoch 140/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 608ms/step - loss: 0.0375 - learning_rate: 1.0000e-04\n",
      "Epoch 141/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 606ms/step - loss: 0.0368 - learning_rate: 1.0000e-04\n",
      "Epoch 142/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 605ms/step - loss: 0.0368 - learning_rate: 1.0000e-04\n",
      "Epoch 143/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 605ms/step - loss: 0.0378 - learning_rate: 1.0000e-04\n",
      "Epoch 144/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 606ms/step - loss: 0.0375 - learning_rate: 1.0000e-04\n",
      "Epoch 145/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 605ms/step - loss: 0.0374 - learning_rate: 1.0000e-04\n",
      "Epoch 146/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 604ms/step - loss: 0.0369 - learning_rate: 1.0000e-04\n",
      "Epoch 147/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 606ms/step - loss: 0.0371 - learning_rate: 1.0000e-04\n",
      "Epoch 148/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 605ms/step - loss: 0.0373 - learning_rate: 1.0000e-04\n",
      "Epoch 149/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 605ms/step - loss: 0.0372 - learning_rate: 1.0000e-04\n",
      "Epoch 150/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 605ms/step - loss: 0.0366 - learning_rate: 1.0000e-04\n",
      "Epoch 151/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 607ms/step - loss: 0.0371 - learning_rate: 1.0000e-04\n",
      "Epoch 152/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 606ms/step - loss: 0.0377 - learning_rate: 1.0000e-04\n",
      "Epoch 153/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 607ms/step - loss: 0.0367 - learning_rate: 1.0000e-04\n",
      "Epoch 154/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 605ms/step - loss: 0.0373 - learning_rate: 1.0000e-04\n",
      "Epoch 155/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 605ms/step - loss: 0.0367 - learning_rate: 1.0000e-04\n",
      "Epoch 156/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 609ms/step - loss: 0.0376 - learning_rate: 1.0000e-04\n",
      "Epoch 157/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 605ms/step - loss: 0.0369 - learning_rate: 1.0000e-04\n",
      "Epoch 158/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 604ms/step - loss: 0.0376 - learning_rate: 1.0000e-04\n",
      "Epoch 159/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 605ms/step - loss: 0.0375 - learning_rate: 1.0000e-04\n",
      "Epoch 160/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 604ms/step - loss: 0.0377 - learning_rate: 1.0000e-05\n",
      "Epoch 161/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 605ms/step - loss: 0.0375 - learning_rate: 1.0000e-05\n",
      "Epoch 162/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 604ms/step - loss: 0.0369 - learning_rate: 1.0000e-05\n",
      "Epoch 163/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 604ms/step - loss: 0.0374 - learning_rate: 1.0000e-05\n",
      "Epoch 164/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 605ms/step - loss: 0.0367 - learning_rate: 1.0000e-05\n",
      "Epoch 165/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 606ms/step - loss: 0.0365 - learning_rate: 1.0000e-05\n",
      "Epoch 166/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 606ms/step - loss: 0.0370 - learning_rate: 1.0000e-05\n",
      "Epoch 167/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 605ms/step - loss: 0.0368 - learning_rate: 1.0000e-05\n",
      "Epoch 168/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 606ms/step - loss: 0.0367 - learning_rate: 1.0000e-05\n",
      "Epoch 169/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 603ms/step - loss: 0.0370 - learning_rate: 1.0000e-05\n",
      "Epoch 170/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 604ms/step - loss: 0.0364 - learning_rate: 1.0000e-05\n",
      "Epoch 171/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 606ms/step - loss: 0.0370 - learning_rate: 1.0000e-05\n",
      "Epoch 172/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 606ms/step - loss: 0.0365 - learning_rate: 1.0000e-05\n",
      "Epoch 173/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 608ms/step - loss: 0.0372 - learning_rate: 1.0000e-05\n",
      "Epoch 174/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 604ms/step - loss: 0.0365 - learning_rate: 1.0000e-05\n",
      "Epoch 175/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 604ms/step - loss: 0.0370 - learning_rate: 1.0000e-05\n",
      "Epoch 176/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 604ms/step - loss: 0.0368 - learning_rate: 1.0000e-05\n",
      "Epoch 177/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 603ms/step - loss: 0.0363 - learning_rate: 1.0000e-06\n",
      "Epoch 178/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 605ms/step - loss: 0.0362 - learning_rate: 1.0000e-06\n",
      "Epoch 179/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 604ms/step - loss: 0.0373 - learning_rate: 1.0000e-06\n",
      "Epoch 180/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 611ms/step - loss: 0.0369 - learning_rate: 1.0000e-06\n",
      "Epoch 181/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 607ms/step - loss: 0.0365 - learning_rate: 1.0000e-06\n",
      "Epoch 182/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 605ms/step - loss: 0.0372 - learning_rate: 1.0000e-06\n",
      "Epoch 183/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 602ms/step - loss: 0.0363 - learning_rate: 1.0000e-06\n",
      "Epoch 184/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 604ms/step - loss: 0.0365 - learning_rate: 1.0000e-06\n",
      "Epoch 185/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 603ms/step - loss: 0.0367 - learning_rate: 1.0000e-06\n",
      "Epoch 186/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 604ms/step - loss: 0.0366 - learning_rate: 1.0000e-06\n",
      "Epoch 187/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 602ms/step - loss: 0.0365 - learning_rate: 1.0000e-07\n",
      "Epoch 188/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 607ms/step - loss: 0.0373 - learning_rate: 1.0000e-07\n",
      "Epoch 189/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 605ms/step - loss: 0.0368 - learning_rate: 1.0000e-07\n",
      "Epoch 190/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 603ms/step - loss: 0.0365 - learning_rate: 1.0000e-07\n",
      "Epoch 191/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 603ms/step - loss: 0.0366 - learning_rate: 1.0000e-07\n",
      "Epoch 192/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 611ms/step - loss: 0.0368 - learning_rate: 1.0000e-07\n",
      "Epoch 193/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 603ms/step - loss: 0.0370 - learning_rate: 1.0000e-07\n",
      "Epoch 194/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 602ms/step - loss: 0.0365 - learning_rate: 1.0000e-07\n",
      "Epoch 195/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 602ms/step - loss: 0.0363 - learning_rate: 1.0000e-07\n",
      "Epoch 196/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 602ms/step - loss: 0.0366 - learning_rate: 1.0000e-07\n",
      "Epoch 197/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 602ms/step - loss: 0.0371 - learning_rate: 1.0000e-07\n",
      "Epoch 198/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 600ms/step - loss: 0.0366 - learning_rate: 1.0000e-07\n",
      "Epoch 199/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 599ms/step - loss: 0.0368 - learning_rate: 1.0000e-07\n",
      "Epoch 200/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 599ms/step - loss: 0.0361 - learning_rate: 1.0000e-07\n",
      "Epoch 201/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 600ms/step - loss: 0.0361 - learning_rate: 1.0000e-07\n",
      "Epoch 202/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 600ms/step - loss: 0.0365 - learning_rate: 1.0000e-07\n",
      "Epoch 203/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 601ms/step - loss: 0.0362 - learning_rate: 1.0000e-07\n",
      "Epoch 204/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 603ms/step - loss: 0.0363 - learning_rate: 1.0000e-07\n",
      "Epoch 205/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 605ms/step - loss: 0.0371 - learning_rate: 1.0000e-07\n",
      "Epoch 206/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 605ms/step - loss: 0.0368 - learning_rate: 1.0000e-07\n",
      "Epoch 207/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 602ms/step - loss: 0.0367 - learning_rate: 1.0000e-07\n",
      "Epoch 208/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 603ms/step - loss: 0.0366 - learning_rate: 1.0000e-07\n",
      "Epoch 209/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 600ms/step - loss: 0.0368 - learning_rate: 1.0000e-07\n",
      "Epoch 210/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 602ms/step - loss: 0.0364 - learning_rate: 1.0000e-07\n",
      "Epoch 211/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 603ms/step - loss: 0.0371 - learning_rate: 1.0000e-07\n",
      "Epoch 212/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 603ms/step - loss: 0.0367 - learning_rate: 1.0000e-07\n",
      "Epoch 213/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 602ms/step - loss: 0.0366 - learning_rate: 1.0000e-07\n",
      "Epoch 214/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 603ms/step - loss: 0.0369 - learning_rate: 1.0000e-07\n",
      "Epoch 215/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 602ms/step - loss: 0.0367 - learning_rate: 1.0000e-07\n",
      "Epoch 216/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 602ms/step - loss: 0.0365 - learning_rate: 1.0000e-07\n",
      "Epoch 217/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 602ms/step - loss: 0.0368 - learning_rate: 1.0000e-07\n",
      "Epoch 218/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 603ms/step - loss: 0.0363 - learning_rate: 1.0000e-07\n",
      "Epoch 219/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 603ms/step - loss: 0.0366 - learning_rate: 1.0000e-07\n",
      "Epoch 220/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 603ms/step - loss: 0.0365 - learning_rate: 1.0000e-07\n",
      "Epoch 221/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 602ms/step - loss: 0.0364 - learning_rate: 1.0000e-07\n",
      "Epoch 222/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 606ms/step - loss: 0.0367 - learning_rate: 1.0000e-07\n",
      "Epoch 223/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 603ms/step - loss: 0.0367 - learning_rate: 1.0000e-07\n",
      "Epoch 224/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 603ms/step - loss: 0.0363 - learning_rate: 1.0000e-07\n",
      "Epoch 225/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 603ms/step - loss: 0.0366 - learning_rate: 1.0000e-07\n",
      "Epoch 226/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 603ms/step - loss: 0.0363 - learning_rate: 1.0000e-07\n",
      "Epoch 227/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 606ms/step - loss: 0.0361 - learning_rate: 1.0000e-07\n",
      "Epoch 228/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 602ms/step - loss: 0.0368 - learning_rate: 1.0000e-07\n",
      "Epoch 229/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 603ms/step - loss: 0.0364 - learning_rate: 1.0000e-07\n",
      "Epoch 230/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 600ms/step - loss: 0.0364 - learning_rate: 1.0000e-07\n",
      "Epoch 231/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 603ms/step - loss: 0.0368 - learning_rate: 1.0000e-07\n",
      "Epoch 232/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 602ms/step - loss: 0.0366 - learning_rate: 1.0000e-07\n",
      "Epoch 233/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 603ms/step - loss: 0.0363 - learning_rate: 1.0000e-07\n",
      "Epoch 234/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 606ms/step - loss: 0.0370 - learning_rate: 1.0000e-07\n",
      "Epoch 235/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 603ms/step - loss: 0.0365 - learning_rate: 1.0000e-07\n",
      "Epoch 236/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 601ms/step - loss: 0.0373 - learning_rate: 1.0000e-07\n",
      "Epoch 237/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 602ms/step - loss: 0.0365 - learning_rate: 1.0000e-07\n",
      "Epoch 238/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 603ms/step - loss: 0.0366 - learning_rate: 1.0000e-07\n",
      "Epoch 239/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 603ms/step - loss: 0.0366 - learning_rate: 1.0000e-07\n",
      "Epoch 240/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 602ms/step - loss: 0.0367 - learning_rate: 1.0000e-07\n",
      "Epoch 241/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 606ms/step - loss: 0.0365 - learning_rate: 1.0000e-07\n",
      "Epoch 242/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 604ms/step - loss: 0.0370 - learning_rate: 1.0000e-07\n",
      "Epoch 243/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 601ms/step - loss: 0.0369 - learning_rate: 1.0000e-07\n",
      "Epoch 244/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 602ms/step - loss: 0.0371 - learning_rate: 1.0000e-07\n",
      "Epoch 245/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 602ms/step - loss: 0.0365 - learning_rate: 1.0000e-07\n",
      "Epoch 246/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 604ms/step - loss: 0.0363 - learning_rate: 1.0000e-07\n",
      "Epoch 247/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 603ms/step - loss: 0.0363 - learning_rate: 1.0000e-07\n",
      "Epoch 248/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 603ms/step - loss: 0.0370 - learning_rate: 1.0000e-07\n",
      "Epoch 249/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 602ms/step - loss: 0.0362 - learning_rate: 1.0000e-07\n",
      "Epoch 250/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 603ms/step - loss: 0.0370 - learning_rate: 1.0000e-07\n",
      "Epoch 251/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 602ms/step - loss: 0.0365 - learning_rate: 1.0000e-07\n",
      "Epoch 252/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 604ms/step - loss: 0.0360 - learning_rate: 1.0000e-07\n",
      "Epoch 253/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 610ms/step - loss: 0.0367 - learning_rate: 1.0000e-07\n",
      "Epoch 254/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 602ms/step - loss: 0.0368 - learning_rate: 1.0000e-07\n",
      "Epoch 255/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 606ms/step - loss: 0.0367 - learning_rate: 1.0000e-07\n",
      "Epoch 256/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 602ms/step - loss: 0.0365 - learning_rate: 1.0000e-07\n",
      "Epoch 257/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 602ms/step - loss: 0.0364 - learning_rate: 1.0000e-07\n",
      "Epoch 258/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 602ms/step - loss: 0.0367 - learning_rate: 1.0000e-07\n",
      "Epoch 259/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 601ms/step - loss: 0.0367 - learning_rate: 1.0000e-07\n",
      "Epoch 260/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 602ms/step - loss: 0.0367 - learning_rate: 1.0000e-07\n",
      "Epoch 261/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 603ms/step - loss: 0.0366 - learning_rate: 1.0000e-07\n",
      "Epoch 262/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 599ms/step - loss: 0.0365 - learning_rate: 1.0000e-07\n",
      "Epoch 263/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 597ms/step - loss: 0.0368 - learning_rate: 1.0000e-07\n",
      "Epoch 264/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 599ms/step - loss: 0.0373 - learning_rate: 1.0000e-07\n",
      "Epoch 265/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 598ms/step - loss: 0.0365 - learning_rate: 1.0000e-07\n",
      "Epoch 266/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 598ms/step - loss: 0.0366 - learning_rate: 1.0000e-07\n",
      "Epoch 267/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 600ms/step - loss: 0.0372 - learning_rate: 1.0000e-07\n",
      "Epoch 268/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 598ms/step - loss: 0.0363 - learning_rate: 1.0000e-07\n",
      "Epoch 269/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 597ms/step - loss: 0.0368 - learning_rate: 1.0000e-07\n",
      "Epoch 270/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 598ms/step - loss: 0.0366 - learning_rate: 1.0000e-07\n",
      "Epoch 271/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 599ms/step - loss: 0.0363 - learning_rate: 1.0000e-07\n",
      "Epoch 272/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 601ms/step - loss: 0.0366 - learning_rate: 1.0000e-07\n",
      "Epoch 273/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 601ms/step - loss: 0.0367 - learning_rate: 1.0000e-07\n",
      "Epoch 274/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 598ms/step - loss: 0.0363 - learning_rate: 1.0000e-07\n",
      "Epoch 275/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 598ms/step - loss: 0.0368 - learning_rate: 1.0000e-07\n",
      "Epoch 276/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 602ms/step - loss: 0.0371 - learning_rate: 1.0000e-07\n",
      "Epoch 277/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 599ms/step - loss: 0.0372 - learning_rate: 1.0000e-07\n",
      "Epoch 278/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 599ms/step - loss: 0.0370 - learning_rate: 1.0000e-07\n",
      "Epoch 279/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 599ms/step - loss: 0.0365 - learning_rate: 1.0000e-07\n",
      "Epoch 280/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 598ms/step - loss: 0.0365 - learning_rate: 1.0000e-07\n",
      "Epoch 281/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 599ms/step - loss: 0.0372 - learning_rate: 1.0000e-07\n",
      "Epoch 282/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 598ms/step - loss: 0.0366 - learning_rate: 1.0000e-07\n",
      "Epoch 283/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 598ms/step - loss: 0.0367 - learning_rate: 1.0000e-07\n",
      "Epoch 284/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 598ms/step - loss: 0.0365 - learning_rate: 1.0000e-07\n",
      "Epoch 285/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 601ms/step - loss: 0.0370 - learning_rate: 1.0000e-07\n",
      "Epoch 286/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 603ms/step - loss: 0.0362 - learning_rate: 1.0000e-07\n",
      "Epoch 287/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 602ms/step - loss: 0.0366 - learning_rate: 1.0000e-07\n",
      "Epoch 288/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 611ms/step - loss: 0.0365 - learning_rate: 1.0000e-07\n",
      "Epoch 289/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 615ms/step - loss: 0.0366 - learning_rate: 1.0000e-07\n",
      "Epoch 290/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 592ms/step - loss: 0.0367 - learning_rate: 1.0000e-07\n",
      "Epoch 291/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 590ms/step - loss: 0.0367 - learning_rate: 1.0000e-07\n",
      "Epoch 292/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 590ms/step - loss: 0.0368 - learning_rate: 1.0000e-07\n",
      "Epoch 293/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 587ms/step - loss: 0.0369 - learning_rate: 1.0000e-07\n",
      "Epoch 294/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 587ms/step - loss: 0.0372 - learning_rate: 1.0000e-07\n",
      "Epoch 295/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 592ms/step - loss: 0.0370 - learning_rate: 1.0000e-07\n",
      "Epoch 296/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 596ms/step - loss: 0.0363 - learning_rate: 1.0000e-07\n",
      "Epoch 297/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 618ms/step - loss: 0.0365 - learning_rate: 1.0000e-07\n",
      "Epoch 298/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 600ms/step - loss: 0.0374 - learning_rate: 1.0000e-07\n",
      "Epoch 299/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 599ms/step - loss: 0.0369 - learning_rate: 1.0000e-07\n",
      "Epoch 300/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 595ms/step - loss: 0.0369 - learning_rate: 1.0000e-07\n",
      "Epoch 301/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 588ms/step - loss: 0.0366 - learning_rate: 1.0000e-07\n",
      "Epoch 302/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 590ms/step - loss: 0.0367 - learning_rate: 1.0000e-07\n",
      "Epoch 303/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 593ms/step - loss: 0.0366 - learning_rate: 1.0000e-07\n",
      "Epoch 304/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 592ms/step - loss: 0.0360 - learning_rate: 1.0000e-07\n",
      "Epoch 305/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 591ms/step - loss: 0.0371 - learning_rate: 1.0000e-07\n",
      "Epoch 306/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 590ms/step - loss: 0.0368 - learning_rate: 1.0000e-07\n",
      "Epoch 307/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 588ms/step - loss: 0.0367 - learning_rate: 1.0000e-07\n",
      "Epoch 308/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 589ms/step - loss: 0.0365 - learning_rate: 1.0000e-07\n",
      "Epoch 309/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 591ms/step - loss: 0.0366 - learning_rate: 1.0000e-07\n",
      "Epoch 310/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 592ms/step - loss: 0.0365 - learning_rate: 1.0000e-07\n",
      "Epoch 311/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 592ms/step - loss: 0.0369 - learning_rate: 1.0000e-07\n",
      "Epoch 312/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 590ms/step - loss: 0.0365 - learning_rate: 1.0000e-07\n",
      "Epoch 313/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 589ms/step - loss: 0.0362 - learning_rate: 1.0000e-07\n",
      "Epoch 314/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 598ms/step - loss: 0.0368 - learning_rate: 1.0000e-07\n",
      "Epoch 315/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 589ms/step - loss: 0.0362 - learning_rate: 1.0000e-07\n",
      "Epoch 316/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 588ms/step - loss: 0.0359 - learning_rate: 1.0000e-07\n",
      "Epoch 317/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 591ms/step - loss: 0.0364 - learning_rate: 1.0000e-07\n",
      "Epoch 318/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 596ms/step - loss: 0.0365 - learning_rate: 1.0000e-07\n",
      "Epoch 319/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 587ms/step - loss: 0.0362 - learning_rate: 1.0000e-07\n",
      "Epoch 320/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 589ms/step - loss: 0.0372 - learning_rate: 1.0000e-07\n",
      "Epoch 321/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 590ms/step - loss: 0.0361 - learning_rate: 1.0000e-07\n",
      "Epoch 322/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 593ms/step - loss: 0.0365 - learning_rate: 1.0000e-07\n",
      "Epoch 323/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 589ms/step - loss: 0.0365 - learning_rate: 1.0000e-07\n",
      "Epoch 324/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 592ms/step - loss: 0.0365 - learning_rate: 1.0000e-07\n",
      "Epoch 325/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 589ms/step - loss: 0.0363 - learning_rate: 1.0000e-07\n",
      "Epoch 326/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 588ms/step - loss: 0.0371 - learning_rate: 1.0000e-07\n",
      "Epoch 327/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 587ms/step - loss: 0.0365 - learning_rate: 1.0000e-07\n",
      "Epoch 328/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 588ms/step - loss: 0.0367 - learning_rate: 1.0000e-07\n",
      "Epoch 329/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 589ms/step - loss: 0.0369 - learning_rate: 1.0000e-07\n",
      "Epoch 330/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 588ms/step - loss: 0.0367 - learning_rate: 1.0000e-07\n",
      "Epoch 331/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 589ms/step - loss: 0.0361 - learning_rate: 1.0000e-07\n",
      "Epoch 332/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 590ms/step - loss: 0.0361 - learning_rate: 1.0000e-07\n",
      "Epoch 333/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 589ms/step - loss: 0.0368 - learning_rate: 1.0000e-07\n",
      "Epoch 334/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 590ms/step - loss: 0.0368 - learning_rate: 1.0000e-07\n",
      "Epoch 335/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 588ms/step - loss: 0.0369 - learning_rate: 1.0000e-07\n",
      "Epoch 336/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 588ms/step - loss: 0.0363 - learning_rate: 1.0000e-07\n",
      "Epoch 337/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 590ms/step - loss: 0.0369 - learning_rate: 1.0000e-07\n",
      "Epoch 338/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 590ms/step - loss: 0.0373 - learning_rate: 1.0000e-07\n",
      "Epoch 339/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 591ms/step - loss: 0.0363 - learning_rate: 1.0000e-07\n",
      "Epoch 340/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 589ms/step - loss: 0.0360 - learning_rate: 1.0000e-07\n",
      "Epoch 341/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 588ms/step - loss: 0.0364 - learning_rate: 1.0000e-07\n",
      "Epoch 342/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 589ms/step - loss: 0.0370 - learning_rate: 1.0000e-07\n",
      "Epoch 343/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 588ms/step - loss: 0.0362 - learning_rate: 1.0000e-07\n",
      "Epoch 344/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 588ms/step - loss: 0.0366 - learning_rate: 1.0000e-07\n",
      "Epoch 345/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 589ms/step - loss: 0.0372 - learning_rate: 1.0000e-07\n",
      "Epoch 346/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 589ms/step - loss: 0.0368 - learning_rate: 1.0000e-07\n",
      "Epoch 347/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 588ms/step - loss: 0.0367 - learning_rate: 1.0000e-07\n",
      "Epoch 348/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 591ms/step - loss: 0.0362 - learning_rate: 1.0000e-07\n",
      "Epoch 349/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 593ms/step - loss: 0.0365 - learning_rate: 1.0000e-07\n",
      "Epoch 350/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 593ms/step - loss: 0.0370 - learning_rate: 1.0000e-07\n",
      "Epoch 351/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 591ms/step - loss: 0.0369 - learning_rate: 1.0000e-07\n",
      "Epoch 352/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 592ms/step - loss: 0.0367 - learning_rate: 1.0000e-07\n",
      "Epoch 353/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 591ms/step - loss: 0.0370 - learning_rate: 1.0000e-07\n",
      "Epoch 354/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 593ms/step - loss: 0.0368 - learning_rate: 1.0000e-07\n",
      "Epoch 355/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 594ms/step - loss: 0.0365 - learning_rate: 1.0000e-07\n",
      "Epoch 356/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 593ms/step - loss: 0.0360 - learning_rate: 1.0000e-07\n",
      "Epoch 357/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 596ms/step - loss: 0.0366 - learning_rate: 1.0000e-07\n",
      "Epoch 358/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 595ms/step - loss: 0.0368 - learning_rate: 1.0000e-07\n",
      "Epoch 359/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 592ms/step - loss: 0.0361 - learning_rate: 1.0000e-07\n",
      "Epoch 360/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 590ms/step - loss: 0.0366 - learning_rate: 1.0000e-07\n",
      "Epoch 361/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 588ms/step - loss: 0.0371 - learning_rate: 1.0000e-07\n",
      "Epoch 362/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 590ms/step - loss: 0.0363 - learning_rate: 1.0000e-07\n",
      "Epoch 363/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 588ms/step - loss: 0.0364 - learning_rate: 1.0000e-07\n",
      "Epoch 364/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 590ms/step - loss: 0.0361 - learning_rate: 1.0000e-07\n",
      "Epoch 365/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 595ms/step - loss: 0.0372 - learning_rate: 1.0000e-07\n",
      "Epoch 366/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 592ms/step - loss: 0.0359 - learning_rate: 1.0000e-07\n",
      "Epoch 367/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 588ms/step - loss: 0.0366 - learning_rate: 1.0000e-07\n",
      "Epoch 368/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 589ms/step - loss: 0.0369 - learning_rate: 1.0000e-07\n",
      "Epoch 369/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 588ms/step - loss: 0.0366 - learning_rate: 1.0000e-07\n",
      "Epoch 370/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 587ms/step - loss: 0.0368 - learning_rate: 1.0000e-07\n",
      "Epoch 371/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 591ms/step - loss: 0.0366 - learning_rate: 1.0000e-07\n",
      "Epoch 372/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 591ms/step - loss: 0.0363 - learning_rate: 1.0000e-07\n",
      "Epoch 373/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 591ms/step - loss: 0.0369 - learning_rate: 1.0000e-07\n",
      "Epoch 374/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 587ms/step - loss: 0.0364 - learning_rate: 1.0000e-07\n",
      "Epoch 375/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 589ms/step - loss: 0.0369 - learning_rate: 1.0000e-07\n",
      "Epoch 376/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 594ms/step - loss: 0.0371 - learning_rate: 1.0000e-07\n",
      "Epoch 377/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 591ms/step - loss: 0.0370 - learning_rate: 1.0000e-07\n",
      "Epoch 378/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 589ms/step - loss: 0.0366 - learning_rate: 1.0000e-07\n",
      "Epoch 379/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 589ms/step - loss: 0.0367 - learning_rate: 1.0000e-07\n",
      "Epoch 380/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 588ms/step - loss: 0.0371 - learning_rate: 1.0000e-07\n",
      "Epoch 381/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 587ms/step - loss: 0.0368 - learning_rate: 1.0000e-07\n",
      "Epoch 382/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 586ms/step - loss: 0.0369 - learning_rate: 1.0000e-07\n",
      "Epoch 383/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 589ms/step - loss: 0.0366 - learning_rate: 1.0000e-07\n",
      "Epoch 384/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 588ms/step - loss: 0.0369 - learning_rate: 1.0000e-07\n",
      "Epoch 385/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 589ms/step - loss: 0.0361 - learning_rate: 1.0000e-07\n",
      "Epoch 386/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 590ms/step - loss: 0.0365 - learning_rate: 1.0000e-07\n",
      "Epoch 387/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 588ms/step - loss: 0.0369 - learning_rate: 1.0000e-07\n",
      "Epoch 388/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 588ms/step - loss: 0.0364 - learning_rate: 1.0000e-07\n",
      "Epoch 389/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 590ms/step - loss: 0.0359 - learning_rate: 1.0000e-07\n",
      "Epoch 390/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 590ms/step - loss: 0.0368 - learning_rate: 1.0000e-07\n",
      "Epoch 391/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 590ms/step - loss: 0.0366 - learning_rate: 1.0000e-07\n",
      "Epoch 392/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 589ms/step - loss: 0.0363 - learning_rate: 1.0000e-07\n",
      "Epoch 393/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 590ms/step - loss: 0.0363 - learning_rate: 1.0000e-07\n",
      "Epoch 394/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 588ms/step - loss: 0.0368 - learning_rate: 1.0000e-07\n",
      "Epoch 395/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 589ms/step - loss: 0.0367 - learning_rate: 1.0000e-07\n",
      "Epoch 396/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 590ms/step - loss: 0.0362 - learning_rate: 1.0000e-07\n",
      "Epoch 397/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 591ms/step - loss: 0.0367 - learning_rate: 1.0000e-07\n",
      "Epoch 398/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 589ms/step - loss: 0.0370 - learning_rate: 1.0000e-07\n",
      "Epoch 399/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 590ms/step - loss: 0.0363 - learning_rate: 1.0000e-07\n",
      "Epoch 400/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 588ms/step - loss: 0.0367 - learning_rate: 1.0000e-07\n",
      "Epoch 401/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 590ms/step - loss: 0.0365 - learning_rate: 1.0000e-07\n",
      "Epoch 402/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 590ms/step - loss: 0.0367 - learning_rate: 1.0000e-07\n",
      "Epoch 403/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 590ms/step - loss: 0.0367 - learning_rate: 1.0000e-07\n",
      "Epoch 404/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 589ms/step - loss: 0.0368 - learning_rate: 1.0000e-07\n",
      "Epoch 405/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 591ms/step - loss: 0.0367 - learning_rate: 1.0000e-07\n",
      "Epoch 406/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 588ms/step - loss: 0.0368 - learning_rate: 1.0000e-07\n",
      "Epoch 407/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 591ms/step - loss: 0.0370 - learning_rate: 1.0000e-07\n",
      "Epoch 408/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 589ms/step - loss: 0.0368 - learning_rate: 1.0000e-07\n",
      "Epoch 409/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 589ms/step - loss: 0.0370 - learning_rate: 1.0000e-07\n",
      "Epoch 410/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 588ms/step - loss: 0.0363 - learning_rate: 1.0000e-07\n",
      "Epoch 411/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 593ms/step - loss: 0.0371 - learning_rate: 1.0000e-07\n",
      "Epoch 412/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 589ms/step - loss: 0.0367 - learning_rate: 1.0000e-07\n",
      "Epoch 413/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 589ms/step - loss: 0.0366 - learning_rate: 1.0000e-07\n",
      "Epoch 414/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 588ms/step - loss: 0.0363 - learning_rate: 1.0000e-07\n",
      "Epoch 415/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 590ms/step - loss: 0.0367 - learning_rate: 1.0000e-07\n",
      "Epoch 416/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 590ms/step - loss: 0.0362 - learning_rate: 1.0000e-07\n",
      "Epoch 417/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 593ms/step - loss: 0.0359 - learning_rate: 1.0000e-07\n",
      "Epoch 418/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 588ms/step - loss: 0.0361 - learning_rate: 1.0000e-07\n",
      "Epoch 419/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 590ms/step - loss: 0.0368 - learning_rate: 1.0000e-07\n",
      "Epoch 420/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 589ms/step - loss: 0.0368 - learning_rate: 1.0000e-07\n",
      "Epoch 421/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 591ms/step - loss: 0.0370 - learning_rate: 1.0000e-07\n",
      "Epoch 422/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 593ms/step - loss: 0.0371 - learning_rate: 1.0000e-07\n",
      "Epoch 423/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 591ms/step - loss: 0.0369 - learning_rate: 1.0000e-07\n",
      "Epoch 424/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 588ms/step - loss: 0.0369 - learning_rate: 1.0000e-07\n",
      "Epoch 425/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 589ms/step - loss: 0.0367 - learning_rate: 1.0000e-07\n",
      "Epoch 426/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 589ms/step - loss: 0.0367 - learning_rate: 1.0000e-07\n",
      "Epoch 427/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 591ms/step - loss: 0.0367 - learning_rate: 1.0000e-07\n",
      "Epoch 428/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 593ms/step - loss: 0.0365 - learning_rate: 1.0000e-07\n",
      "Epoch 429/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 588ms/step - loss: 0.0367 - learning_rate: 1.0000e-07\n",
      "Epoch 430/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 588ms/step - loss: 0.0366 - learning_rate: 1.0000e-07\n",
      "Epoch 431/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 590ms/step - loss: 0.0367 - learning_rate: 1.0000e-07\n",
      "Epoch 432/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 591ms/step - loss: 0.0366 - learning_rate: 1.0000e-07\n",
      "Epoch 433/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 594ms/step - loss: 0.0365 - learning_rate: 1.0000e-07\n",
      "Epoch 434/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 598ms/step - loss: 0.0368 - learning_rate: 1.0000e-07\n",
      "Epoch 435/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 590ms/step - loss: 0.0363 - learning_rate: 1.0000e-07\n",
      "Epoch 436/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 588ms/step - loss: 0.0367 - learning_rate: 1.0000e-07\n",
      "Epoch 437/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 588ms/step - loss: 0.0367 - learning_rate: 1.0000e-07\n",
      "Epoch 438/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 588ms/step - loss: 0.0365 - learning_rate: 1.0000e-07\n",
      "Epoch 439/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 598ms/step - loss: 0.0366 - learning_rate: 1.0000e-07\n",
      "Epoch 440/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 591ms/step - loss: 0.0370 - learning_rate: 1.0000e-07\n",
      "Epoch 441/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 587ms/step - loss: 0.0365 - learning_rate: 1.0000e-07\n",
      "Epoch 442/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 587ms/step - loss: 0.0365 - learning_rate: 1.0000e-07\n",
      "Epoch 443/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 586ms/step - loss: 0.0370 - learning_rate: 1.0000e-07\n",
      "Epoch 444/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 588ms/step - loss: 0.0361 - learning_rate: 1.0000e-07\n",
      "Epoch 445/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 588ms/step - loss: 0.0371 - learning_rate: 1.0000e-07\n",
      "Epoch 446/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 589ms/step - loss: 0.0366 - learning_rate: 1.0000e-07\n",
      "Epoch 447/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 587ms/step - loss: 0.0366 - learning_rate: 1.0000e-07\n",
      "Epoch 448/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 587ms/step - loss: 0.0371 - learning_rate: 1.0000e-07\n",
      "Epoch 449/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 586ms/step - loss: 0.0364 - learning_rate: 1.0000e-07\n",
      "Epoch 450/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 587ms/step - loss: 0.0369 - learning_rate: 1.0000e-07\n",
      "Epoch 451/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 587ms/step - loss: 0.0371 - learning_rate: 1.0000e-07\n",
      "Epoch 452/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 587ms/step - loss: 0.0365 - learning_rate: 1.0000e-07\n",
      "Epoch 453/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 589ms/step - loss: 0.0364 - learning_rate: 1.0000e-07\n",
      "Epoch 454/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 587ms/step - loss: 0.0366 - learning_rate: 1.0000e-07\n",
      "Epoch 455/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 586ms/step - loss: 0.0363 - learning_rate: 1.0000e-07\n",
      "Epoch 456/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 589ms/step - loss: 0.0374 - learning_rate: 1.0000e-07\n",
      "Epoch 457/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 590ms/step - loss: 0.0371 - learning_rate: 1.0000e-07\n",
      "Epoch 458/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 589ms/step - loss: 0.0367 - learning_rate: 1.0000e-07\n",
      "Epoch 459/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 589ms/step - loss: 0.0365 - learning_rate: 1.0000e-07\n",
      "Epoch 460/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 590ms/step - loss: 0.0367 - learning_rate: 1.0000e-07\n",
      "Epoch 461/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 587ms/step - loss: 0.0366 - learning_rate: 1.0000e-07\n",
      "Epoch 462/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 588ms/step - loss: 0.0364 - learning_rate: 1.0000e-07\n",
      "Epoch 463/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 589ms/step - loss: 0.0364 - learning_rate: 1.0000e-07\n",
      "Epoch 464/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 588ms/step - loss: 0.0367 - learning_rate: 1.0000e-07\n",
      "Epoch 465/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 588ms/step - loss: 0.0368 - learning_rate: 1.0000e-07\n",
      "Epoch 466/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 592ms/step - loss: 0.0366 - learning_rate: 1.0000e-07\n",
      "Epoch 467/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 586ms/step - loss: 0.0374 - learning_rate: 1.0000e-07\n",
      "Epoch 468/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 589ms/step - loss: 0.0364 - learning_rate: 1.0000e-07\n",
      "Epoch 469/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 589ms/step - loss: 0.0364 - learning_rate: 1.0000e-07\n",
      "Epoch 470/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 589ms/step - loss: 0.0362 - learning_rate: 1.0000e-07\n",
      "Epoch 471/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 589ms/step - loss: 0.0367 - learning_rate: 1.0000e-07\n",
      "Epoch 472/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 589ms/step - loss: 0.0368 - learning_rate: 1.0000e-07\n",
      "Epoch 473/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 590ms/step - loss: 0.0364 - learning_rate: 1.0000e-07\n",
      "Epoch 474/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 590ms/step - loss: 0.0365 - learning_rate: 1.0000e-07\n",
      "Epoch 475/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 590ms/step - loss: 0.0364 - learning_rate: 1.0000e-07\n",
      "Epoch 476/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 587ms/step - loss: 0.0363 - learning_rate: 1.0000e-07\n",
      "Epoch 477/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 588ms/step - loss: 0.0368 - learning_rate: 1.0000e-07\n",
      "Epoch 478/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 587ms/step - loss: 0.0372 - learning_rate: 1.0000e-07\n",
      "Epoch 479/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 587ms/step - loss: 0.0368 - learning_rate: 1.0000e-07\n",
      "Epoch 480/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 589ms/step - loss: 0.0371 - learning_rate: 1.0000e-07\n",
      "Epoch 481/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 589ms/step - loss: 0.0371 - learning_rate: 1.0000e-07\n",
      "Epoch 482/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 590ms/step - loss: 0.0366 - learning_rate: 1.0000e-07\n",
      "Epoch 483/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 589ms/step - loss: 0.0372 - learning_rate: 1.0000e-07\n",
      "Epoch 484/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 588ms/step - loss: 0.0368 - learning_rate: 1.0000e-07\n",
      "Epoch 485/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 589ms/step - loss: 0.0371 - learning_rate: 1.0000e-07\n",
      "Epoch 486/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 589ms/step - loss: 0.0366 - learning_rate: 1.0000e-07\n",
      "Epoch 487/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 589ms/step - loss: 0.0367 - learning_rate: 1.0000e-07\n",
      "Epoch 488/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 590ms/step - loss: 0.0364 - learning_rate: 1.0000e-07\n",
      "Epoch 489/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 590ms/step - loss: 0.0367 - learning_rate: 1.0000e-07\n",
      "Epoch 490/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 597ms/step - loss: 0.0364 - learning_rate: 1.0000e-07\n",
      "Epoch 491/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 589ms/step - loss: 0.0368 - learning_rate: 1.0000e-07\n",
      "Epoch 492/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 586ms/step - loss: 0.0368 - learning_rate: 1.0000e-07\n",
      "Epoch 493/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 588ms/step - loss: 0.0368 - learning_rate: 1.0000e-07\n",
      "Epoch 494/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 589ms/step - loss: 0.0365 - learning_rate: 1.0000e-07\n",
      "Epoch 495/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 588ms/step - loss: 0.0364 - learning_rate: 1.0000e-07\n",
      "Epoch 496/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 591ms/step - loss: 0.0366 - learning_rate: 1.0000e-07\n",
      "Epoch 497/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 589ms/step - loss: 0.0364 - learning_rate: 1.0000e-07\n",
      "Epoch 498/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 588ms/step - loss: 0.0371 - learning_rate: 1.0000e-07\n",
      "Epoch 499/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 589ms/step - loss: 0.0365 - learning_rate: 1.0000e-07\n",
      "Epoch 500/500\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 589ms/step - loss: 0.0368 - learning_rate: 1.0000e-07\n",
      "{'loss': [2.9773943424224854, 1.387514591217041, 1.3607184886932373, 1.296921730041504, 1.155626893043518, 1.0140539407730103, 0.8934836387634277, 0.7848768830299377, 0.6906959414482117, 0.6042752861976624, 0.5267953872680664, 0.4595645070075989, 0.39842769503593445, 0.3439202308654785, 0.297759085893631, 0.26079273223876953, 0.22981998324394226, 0.20490171015262604, 0.18402984738349915, 0.1676775962114334, 0.15490129590034485, 0.14403226971626282, 0.13390959799289703, 0.1262596994638443, 0.12077438831329346, 0.1148938462138176, 0.10942501574754715, 0.1049489751458168, 0.10133326798677444, 0.09838815033435822, 0.09570760279893875, 0.09460411220788956, 0.0913524404168129, 0.08978519588708878, 0.08767999708652496, 0.0861884281039238, 0.08320444822311401, 0.08332471549510956, 0.08189032971858978, 0.08061817288398743, 0.07892639935016632, 0.077958844602108, 0.07800765335559845, 0.07792258262634277, 0.0764194130897522, 0.07639216631650925, 0.07612106949090958, 0.07521632313728333, 0.07489419728517532, 0.07425404340028763, 0.07392704486846924, 0.07287943363189697, 0.07337623089551926, 0.07357602566480637, 0.07320035994052887, 0.0721382200717926, 0.07152958959341049, 0.07205502688884735, 0.0704493597149849, 0.0697263851761818, 0.06954600661993027, 0.0692935511469841, 0.06961525231599808, 0.06978603452444077, 0.0692550390958786, 0.06898815184831619, 0.068899966776371, 0.06849773228168488, 0.06744681298732758, 0.0672263354063034, 0.06747858226299286, 0.06710030138492584, 0.06762626022100449, 0.06686362624168396, 0.06675086915493011, 0.06611453741788864, 0.06617572158575058, 0.06554604321718216, 0.06496275216341019, 0.06451015174388885, 0.06391424685716629, 0.06374987959861755, 0.0641813576221466, 0.06416762620210648, 0.06364532560110092, 0.06312425434589386, 0.06319580227136612, 0.06253311783075333, 0.06440884619951248, 0.06374381482601166, 0.06326533854007721, 0.06306366622447968, 0.06322444975376129, 0.06336008012294769, 0.06299404799938202, 0.06323564797639847, 0.06300073862075806, 0.06176871433854103, 0.06163451075553894, 0.06104883924126625, 0.06134897470474243, 0.06169469654560089, 0.0606737956404686, 0.061064209789037704, 0.06118769198656082, 0.06061520427465439, 0.0605631060898304, 0.060039058327674866, 0.0600910447537899, 0.05926496908068657, 0.05924442782998085, 0.05911150947213173, 0.0589083731174469, 0.0589890256524086, 0.05884138122200966, 0.05888808146119118, 0.05917248874902725, 0.05909448117017746, 0.0596790611743927, 0.060303688049316406, 0.06072395667433739, 0.06053943932056427, 0.06057174876332283, 0.0563451424241066, 0.04850827157497406, 0.04481710493564606, 0.04254506900906563, 0.04105738177895546, 0.04007197171449661, 0.03939405456185341, 0.0389128252863884, 0.03856797516345978, 0.03830753266811371, 0.03810911253094673, 0.03794986009597778, 0.03781946375966072, 0.03770961984992027, 0.03761443495750427, 0.03753119334578514, 0.03745739161968231, 0.037389375269412994, 0.03732826188206673, 0.03727255016565323, 0.037222832441329956, 0.03717581555247307, 0.03713061660528183, 0.03709212318062782, 0.037052419036626816, 0.03702179342508316, 0.03700152412056923, 0.036994900554418564, 0.03700583055615425, 0.037050243467092514, 0.03712446615099907, 0.03724977374076843, 0.03737353906035423, 0.037503890693187714, 0.03756643459200859, 0.03752285987138748, 0.03736915439367294, 0.037104491144418716, 0.03696822747588158, 0.03688374161720276, 0.03682902082800865, 0.03679240494966507, 0.03676673024892807, 0.03674819692969322, 0.036734603345394135, 0.03672425448894501, 0.03671605512499809, 0.036709342151880264, 0.03670363873243332, 0.03669864311814308, 0.03669402748346329, 0.03668970614671707, 0.03668556734919548, 0.036682046949863434, 0.036681629717350006, 0.03668120875954628, 0.03668077290058136, 0.03668031096458435, 0.03667984530329704, 0.03667936474084854, 0.03667884320020676, 0.036678314208984375, 0.0366777665913105, 0.036677286028862, 0.03667723014950752, 0.03667718544602394, 0.03667711839079857, 0.03667706623673439, 0.03667699918150902, 0.03667692840099335, 0.036676857620477676, 0.0366767942905426, 0.036676712334156036, 0.03667663782835007, 0.0366765633225441, 0.03667647764086723, 0.036676399409770966, 0.03667629882693291, 0.036676209419965744, 0.03667612746357918, 0.03667601943016052, 0.03667593002319336, 0.0366758368909359, 0.036675721406936646, 0.03667562082409859, 0.03667551651597023, 0.036675386130809784, 0.036675285547971725, 0.036675162613391876, 0.03667505457997322, 0.03667491301894188, 0.03667479753494263, 0.03667466342449188, 0.036674536764621735, 0.03667440637946129, 0.036674268543720245, 0.036674149334430695, 0.03667399659752846, 0.03667386621236801, 0.03667372092604637, 0.03667357936501503, 0.03667343407869339, 0.03667328134179115, 0.03667313978075981, 0.036672983318567276, 0.03667283430695534, 0.036672692745923996, 0.03667254000902176, 0.036672383546829224, 0.03667222708463669, 0.03667207062244415, 0.03667192906141281, 0.03667176142334938, 0.036671604961156845, 0.03667144849896431, 0.03667128458619118, 0.036671124398708344, 0.03667096793651581, 0.03667081147432327, 0.03667064383625984, 0.03667048364877701, 0.036670323461294174, 0.03667017072439194, 0.036670006811618805, 0.036669839173555374, 0.03666968643665314, 0.036669522523880005, 0.03666935861110687, 0.03666919842362404, 0.03666902706027031, 0.03666888177394867, 0.03666870668530464, 0.0366685576736927, 0.03666839748620987, 0.03666821867227554, 0.03666805848479271, 0.036667898297309875, 0.03666773810982704, 0.036667581647634506, 0.03666742146015167, 0.036667246371507645, 0.0366671085357666, 0.03666692227125168, 0.036666762083768845, 0.03666660189628601, 0.036666445434093475, 0.03666629269719124, 0.03666611760854721, 0.036665964871644974, 0.03666580468416214, 0.036665644496679306, 0.036665480583906174, 0.03666531667113304, 0.03666515275835991, 0.036664992570877075, 0.03666482865810394, 0.03666466474533081, 0.036664508283138275, 0.03666435554623604, 0.03666418790817261, 0.03666403517127037, 0.03666386380791664, 0.0366637147963047, 0.036663543432950974, 0.03666338324546814, 0.036663226783275604, 0.03666306287050247, 0.03666291385889053, 0.036662742495536804, 0.03666258230805397, 0.03666243329644203, 0.0366622731089592, 0.036662112921476364, 0.036661941558122635, 0.0366617850959301, 0.03666163608431816, 0.03666146472096443, 0.036661311984062195, 0.03666115552186966, 0.036660995334386826, 0.03666084259748459, 0.03666067123413086, 0.03666051849722862, 0.03666036203503609, 0.036660198122262955, 0.03666004538536072, 0.03665989264845848, 0.03665972128510475, 0.036659568548202515, 0.03665940836071968, 0.03665924072265625, 0.03665909543633461, 0.036658938974142075, 0.03665877878665924, 0.03665861859917641, 0.03665846586227417, 0.036658305674791336, 0.0366581529378891, 0.03665800020098686, 0.03665782883763313, 0.03665768355131149, 0.036657530814409256, 0.036657363176345825, 0.03665721043944359, 0.03665705770254135, 0.03665689751505852, 0.036656737327575684, 0.03665658086538315, 0.03665643185377121, 0.036656275391578674, 0.03665611147880554, 0.0366559661924839, 0.036655791103839874, 0.036655645817518234, 0.036655493080616, 0.03665533661842346, 0.03665517643094063, 0.03665502369403839, 0.036654867231845856, 0.03665471449494362, 0.03665455803275108, 0.03665439411997795, 0.036654241383075714, 0.036654096096754074, 0.03665393218398094, 0.0366537868976593, 0.036653630435466766, 0.03665347397327423, 0.0366533026099205, 0.03665315732359886, 0.03665299713611603, 0.036652859300374985, 0.036652691662311554, 0.036652546375989914, 0.03665238618850708, 0.036652226001024246, 0.036652080714702606, 0.03665192797780037, 0.03665177896618843, 0.0366516187787056, 0.03665146976709366, 0.03665131703019142, 0.03665115311741829, 0.03665100038051605, 0.036650851368904114, 0.03665069863200188, 0.03665054216980934, 0.0366503931581974, 0.03665023669600487, 0.03665009140968323, 0.03664993494749069, 0.03664977103471756, 0.036649636924266815, 0.036649465560913086, 0.036649320274591446, 0.03664917126297951, 0.03664901852607727, 0.03664885833859444, 0.03664873167872429, 0.03664856404066086, 0.036648403853178024, 0.03664824739098549, 0.03664809465408325, 0.036647945642471313, 0.03664780780673027, 0.036647647619247437, 0.0366474911570549, 0.03664734214544296, 0.03664718568325043, 0.03664703667163849, 0.036646876484155655, 0.03664674237370491, 0.036646582186222076, 0.03664644435048103, 0.0366462804377079, 0.036646127700805664, 0.03664597496390343, 0.03664582967758179, 0.03664567694067955, 0.03664552420377731, 0.03664537891745567, 0.036645226180553436, 0.0366450697183609, 0.03664492443203926, 0.036644771695137024, 0.03664461895823479, 0.036644477397203445, 0.036644332110881805, 0.03664417192339897, 0.03664401173591614, 0.03664388135075569, 0.03664373233914375, 0.036643579602241516, 0.03664342686533928, 0.03664328530430794, 0.036643121391534805, 0.03664297237992287, 0.03664282709360123, 0.03664267435669899, 0.036642517894506454, 0.03664238005876541, 0.036642227321863174, 0.03664208576083183, 0.0366419292986393, 0.03664178028702736, 0.03664163872599602, 0.03664149343967438, 0.03664133697748184, 0.03664118051528931, 0.036641038954257965, 0.03664088621735573, 0.03664074093103409, 0.03664059564471245, 0.03664043918251991, 0.03664029389619827, 0.03664014860987663, 0.0366399921476841, 0.036639850586652756, 0.03663969412446022, 0.03663954883813858, 0.03663940727710724, 0.0366392582654953, 0.03663911297917366, 0.03663896396756172, 0.03663881495594978, 0.03663866966962814, 0.0366385243833065, 0.03663836792111397, 0.03663822263479233, 0.036638084799051285, 0.03663793206214905, 0.03663777932524681, 0.03663764148950577, 0.03663749247789383, 0.03663734346628189, 0.036637190729379654, 0.036637041717767715, 0.03663690388202667, 0.03663676977157593, 0.036636609584093094, 0.03663645684719086, 0.036636319011449814, 0.036636173725128174, 0.036636024713516235, 0.0366358757019043, 0.03663572669029236, 0.03663558140397072, 0.03663543239235878, 0.03663529455661774, 0.0366351492702961, 0.03663500398397446, 0.03663484752178192, 0.03663470596075058, 0.03663456439971924, 0.036634411662817, 0.03663427755236626, 0.036634113639593124, 0.03663397580385208, 0.03663383796811104, 0.0366336815059185, 0.03663354367017746, 0.03663339093327522, 0.03663325309753418, 0.036633096635341644, 0.0366329699754715, 0.03663282468914986, 0.03663267567753792, 0.03663252666592598, 0.036632388830184937, 0.036632239818573, 0.036632098257541656, 0.03663194924592972, 0.036631811410188675, 0.036631662398576736, 0.036631520837545395, 0.03663136810064316, 0.03663122281432152], 'learning_rate': [0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.00010000000474974513, 0.00010000000474974513, 0.00010000000474974513, 0.00010000000474974513, 0.00010000000474974513, 0.00010000000474974513, 0.00010000000474974513, 0.00010000000474974513, 0.00010000000474974513, 0.00010000000474974513, 0.00010000000474974513, 0.00010000000474974513, 0.00010000000474974513, 0.00010000000474974513, 0.00010000000474974513, 0.00010000000474974513, 0.00010000000474974513, 0.00010000000474974513, 0.00010000000474974513, 0.00010000000474974513, 0.00010000000474974513, 0.00010000000474974513, 0.00010000000474974513, 0.00010000000474974513, 0.00010000000474974513, 0.00010000000474974513, 0.00010000000474974513, 0.00010000000474974513, 0.00010000000474974513, 0.00010000000474974513, 0.00010000000474974513, 0.00010000000474974513, 0.00010000000474974513, 0.00010000000474974513, 0.00010000000474974513, 0.00010000000474974513, 1.0000000656873453e-05, 1.0000000656873453e-05, 1.0000000656873453e-05, 1.0000000656873453e-05, 1.0000000656873453e-05, 1.0000000656873453e-05, 1.0000000656873453e-05, 1.0000000656873453e-05, 1.0000000656873453e-05, 1.0000000656873453e-05, 1.0000000656873453e-05, 1.0000000656873453e-05, 1.0000000656873453e-05, 1.0000000656873453e-05, 1.0000000656873453e-05, 1.0000000656873453e-05, 1.0000000656873453e-05, 1.0000001111620804e-06, 1.0000001111620804e-06, 1.0000001111620804e-06, 1.0000001111620804e-06, 1.0000001111620804e-06, 1.0000001111620804e-06, 1.0000001111620804e-06, 1.0000001111620804e-06, 1.0000001111620804e-06, 1.0000001111620804e-06, 1.000000082740371e-07, 1.000000082740371e-07, 1.000000082740371e-07, 1.000000082740371e-07, 1.000000082740371e-07, 1.000000082740371e-07, 1.000000082740371e-07, 1.000000082740371e-07, 1.000000082740371e-07, 1.000000082740371e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07, 1.0000000116860974e-07]}\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "# Compile the model with optimizer and loss function\n",
    "model.compile(optimizer=Adam(learning_rate=1e-3), \n",
    "              loss=SparseCategoricalCrossentropy(from_logits=False))\n",
    "\n",
    "# Define a learning rate scheduler callback\n",
    "lr_scheduler = ReduceLROnPlateau(monitor='loss', factor=0.1, patience=10, min_lr=1e-7)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit([title_inputs, lyrics_inputs], \n",
    "                    lyrics_outputs, \n",
    "                    epochs=500, \n",
    "                    batch_size=64, \n",
    "                    callbacks=[lr_scheduler], \n",
    "                    verbose=1)\n",
    "\n",
    "# Print training history\n",
    "print(history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model to a file\n",
    "model.save('tf_model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input\n",
    "\n",
    "@register_keras_serializable()\n",
    "class SelfAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.W_q = tf.keras.layers.Dense(d_model)\n",
    "        self.W_k = tf.keras.layers.Dense(d_model)\n",
    "        self.W_v = tf.keras.layers.Dense(d_model)\n",
    "        \n",
    "    def call(self, q, k, v):\n",
    "        scores = tf.matmul(self.W_q(q), self.W_k(k), transpose_b=True)\n",
    "        scaled_scores = scores / tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        attention_weights = tf.nn.softmax(scaled_scores, axis=-1)\n",
    "        output = tf.matmul(attention_weights, self.W_v(v))\n",
    "        return output, attention_weights\n",
    "\n",
    "@register_keras_serializable()\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_heads, d_model):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        self.head_size = d_model // num_heads\n",
    "        self.attention_heads = [SelfAttention(self.head_size) for _ in range(num_heads)]\n",
    "        self.dense = tf.keras.layers.Dense(d_model)\n",
    "        \n",
    "    def call(self, q, k, v):\n",
    "        head_outputs = [head(q, k, v)[0] for head in self.attention_heads]\n",
    "        concatenated = tf.concat(head_outputs, axis=-1)\n",
    "        output = self.dense(concatenated)\n",
    "        return output\n",
    "\n",
    "@register_keras_serializable()\n",
    "class FeedForward(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.dense1 = tf.keras.layers.Dense(d_ff, activation=\"relu\")\n",
    "        self.dense2 = tf.keras.layers.Dense(d_model)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        intermediate_output = self.dense1(inputs)\n",
    "        output = self.dense2(intermediate_output)\n",
    "        return output\n",
    "\n",
    "@register_keras_serializable()\n",
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_heads, d_model, d_ff):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.multihead_attention = MultiHeadAttention(num_heads, d_model)\n",
    "        self.feed_forward = FeedForward(d_model, d_ff)\n",
    "        self.layer_norm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layer_norm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        \n",
    "    def call(self, inputs, training=False):\n",
    "        attn_output = self.multihead_attention(inputs, inputs, inputs)\n",
    "        attn_output = self.layer_norm1(inputs + attn_output)\n",
    "        ff_output = self.feed_forward(attn_output)\n",
    "        output = self.layer_norm2(attn_output + ff_output)\n",
    "        return output\n",
    "\n",
    "@register_keras_serializable()\n",
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_heads, d_model, d_ff):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.masked_self_attention = MultiHeadAttention(num_heads, d_model)\n",
    "        self.cross_attention = MultiHeadAttention(num_heads, d_model)\n",
    "        self.feed_forward = FeedForward(d_model, d_ff)\n",
    "        self.layer_norm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layer_norm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layer_norm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "            \n",
    "    def call(self, inputs, enc_output, training=False):\n",
    "        masked_attn_output = self.masked_self_attention(inputs, inputs, inputs)\n",
    "        masked_attn_output = self.layer_norm1(inputs + masked_attn_output)\n",
    "        cross_attn_output = self.cross_attention(masked_attn_output, enc_output, enc_output)\n",
    "        cross_attn_output = self.layer_norm2(masked_attn_output + cross_attn_output)\n",
    "        ff_output = self.feed_forward(cross_attn_output)\n",
    "        output = self.layer_norm3(cross_attn_output + ff_output)\n",
    "        return output\n",
    "\n",
    "@register_keras_serializable()\n",
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, num_layers, num_heads, d_model, d_ff, input_vocab_size, target_vocab_size, max_seq_len):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder_layers = [EncoderLayer(num_heads, d_model, d_ff) for _ in range(num_layers)]\n",
    "        self.decoder_layers = [DecoderLayer(num_heads, d_model, d_ff) for _ in range(num_layers)]\n",
    "        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
    "        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
    "        self.softmax = tf.keras.layers.Softmax(axis=-1)\n",
    "        self.max_seq_len = max_seq_len\n",
    "        \n",
    "    def call(self, inputs, training=False):\n",
    "        enc_inputs, dec_inputs = inputs\n",
    "        enc_output = self.encode(enc_inputs, training=training)\n",
    "        dec_output = self.decode(dec_inputs, enc_output, training=training)\n",
    "        return dec_output\n",
    "\n",
    "    def encode(self, inputs, training=False):\n",
    "        x = self.embedding(inputs)\n",
    "        for encoder_layer in self.encoder_layers:\n",
    "            x = encoder_layer(x, training=training)\n",
    "        return x\n",
    "    \n",
    "    def decode(self, targets, enc_output, training=False):\n",
    "        x = self.embedding(targets)\n",
    "        for decoder_layer in self.decoder_layers:\n",
    "            x = decoder_layer(x, enc_output, training=training)\n",
    "        dec_output = self.final_layer(x)\n",
    "        dec_output = self.softmax(dec_output)\n",
    "        return dec_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('tf_model1.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "def generate_lyrics(model, tokenizer, title, max_lyrics_len, temperature=1.0, top_k=50):\n",
    "    # Encode the title\n",
    "    title_sequence = tokenizer.texts_to_sequences([title])[0]\n",
    "    title_sequence = np.array(title_sequence).reshape(1, -1)\n",
    "    \n",
    "    # Start with the beginning of sequence token\n",
    "    BOS_token = tokenizer.word_index.get('<bos>', 0)  # Adjust based on your tokenizer\n",
    "    EOS_token = tokenizer.word_index.get('<eos>', 0)  # Adjust based on your tokenizer\n",
    "    decoder_input = [BOS_token]\n",
    "    output_sequence = []\n",
    "    \n",
    "    for i in range(max_lyrics_len):\n",
    "        # Prepare the inputs\n",
    "        decoder_input_array = np.array(decoder_input).reshape(1, -1)\n",
    "        \n",
    "        # Predict the next token\n",
    "        predictions = model([title_sequence, decoder_input_array], training=False)\n",
    "        predictions = predictions[0, -1, :]\n",
    "        \n",
    "        # Apply temperature scaling\n",
    "        predictions = predictions / temperature\n",
    "        \n",
    "        # Clip the predictions to prevent NaN values\n",
    "        predictions = tf.clip_by_value(predictions, -1e9, 1e9)\n",
    "        \n",
    "        # Apply top-k sampling\n",
    "        sorted_indices = tf.argsort(predictions, direction='DESCENDING')[:top_k]\n",
    "        sorted_predictions = tf.gather(predictions, sorted_indices)\n",
    "        \n",
    "        # Check for NaN values\n",
    "        if tf.reduce_any(tf.math.is_nan(sorted_predictions)):\n",
    "            print(\"NaN values found in predictions. Skipping this step.\")\n",
    "            break\n",
    "        \n",
    "        probabilities = tf.nn.softmax(sorted_predictions).numpy()\n",
    "        \n",
    "        # Check for NaN values in probabilities\n",
    "        if np.isnan(probabilities).any():\n",
    "            print(\"NaN values found in probabilities. Skipping this step.\")\n",
    "            break\n",
    "        \n",
    "        next_token = np.random.choice(sorted_indices.numpy(), p=probabilities)\n",
    "        \n",
    "        # Append the token to the output sequence\n",
    "        output_sequence.append(next_token)\n",
    "        \n",
    "        # Check if the end-of-sequence token is generated\n",
    "        if next_token == EOS_token:\n",
    "            break\n",
    "        \n",
    "        # Append the next token to the decoder input\n",
    "        decoder_input.append(next_token)\n",
    "    \n",
    "    # Convert the output sequence to text\n",
    "    generated_lyrics = tokenizer.sequences_to_texts([output_sequence])[0]\n",
    "    \n",
    "    return generated_lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Lyrics: of fire day when only chorus billy l only l make out rest quiet mirror live night never seem clear sunshine s nice stone never start workin weak right rest of together verse control before mirror small trial verse when celebrate faces aw only workin nothing rest whod ne another living together only better summers c shabada make going through to spend weary rocky no my dr control communicating cant barely s seven sale then placing if on living fall through heartache should never reaching my better make clear cant ne ends waste time troubadour please still together only\n"
     ]
    }
   ],
   "source": [
    "title = \"like that\"\n",
    "generated_lyrics = generate_lyrics(transformer, lyrics_tokenizer, title, max_lyrics_len=100)\n",
    "print(f\"Generated Lyrics: {generated_lyrics}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fashion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
